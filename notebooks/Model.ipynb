{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11449575-6314-4708-9193-b279891e836c",
   "metadata": {},
   "source": [
    "## Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f302c0d-40f1-4bac-bf57-df54c89e72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 26 02:41:33 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    39W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ee382-8c34-4130-99d3-80d5abdfbf1a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d150586-d672-4999-9a7f-743a7d0d3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2664559-8789-4785-83d6-26327fe5ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09a35d4-17a4-437a-ad75-cfdbf9215827",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5273f979-011c-4122-930a-126665f3c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import pack, unpack, repeat, reduce, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ef863cc-4128-49a3-a156-fbdc859a9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataloader import BEDataset, BEDataModule\n",
    "import token_learner\n",
    "from transformer import PositionalEncoder, MultiHeadSelfAttention, FeedFowardLayer, generate_masks, generate_causal_attention_mask\n",
    "\n",
    "from film_layers import FiLMBlockV2, FiLMEncoder, ResBlockDWConv\n",
    "from rt1 import RT1Encoder\n",
    "from utils.data_utils import History\n",
    "from utils.model_utils import TextEncoder, ImageFeatureExtractor, plot_self_attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425b526-43d7-483b-b3bc-02028bfb4b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68541d-e1c2-48df-a18d-c37c9da42002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = pd.read_csv(os.path.join(config.DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "# csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820bb68f-3341-4d6e-9cea-80dbdf2de002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # examples: 4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training on 3915 samples.\n",
      "INFO:root:Validating on 961 samples.\n"
     ]
    }
   ],
   "source": [
    "dm = BEDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383eaa10-47d7-4620-928e-8fbcd6889d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*100)\n",
    "# logging.info(\"\\n>> train data loader\")\n",
    "# print(f\"# train batches\\t: {len(dm.train_dataloader())}\")\n",
    "# for data in dm.train_dataloader():\n",
    "#     # pprint(data)\n",
    "#     sample_id, in_state, ad, cmd = data[\"sample_id\"], data[\"in_state\"], data[\"action_desc\"], data[\"motor_cmd\"]\n",
    "#     print(\"In \\t\\t\\t: \", in_state.shape)\n",
    "#     print(\"Action desc \\t\\t: \", ad[\"ids\"].shape)\n",
    "#     print(\"Action desc (len) \\t: \", ad[\"length\"].shape)\n",
    "#     print(\"CMD \\t\\t\\t: \", cmd[\"ids\"].shape)\n",
    "#     print(\"CMD(len) \\t\\t: \", cmd[\"length\"].shape)\n",
    "#     break\n",
    "\n",
    "# print(\"\\nIDs & decided tokens\")\n",
    "# for data in dm.train_dataloader():\n",
    "#     print(data[\"action_desc\"][\"ids\"][0].tolist())\n",
    "#     print(dm.train_ds._decode_inputs(data[\"action_desc\"][\"ids\"][0].tolist()))\n",
    "#     print()\n",
    "#     print(data[\"motor_cmd\"][\"ids\"][0].tolist())\n",
    "#     print(dm.train_ds._decode_outputs(data[\"motor_cmd\"][\"ids\"][0].tolist()))\n",
    "\n",
    "#     break\n",
    "    \n",
    "# print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6bfcb6-3d67-4d4a-934d-95af834e1a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c22a7a-16f8-4411-8839-8b724fc29869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 996 ms, total: 2.04 s\n",
      "Wall time: 4.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 288, 288])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sample = next(iter(dm.train_dataloader()))\n",
    "sample[\"in_state\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b32ff7-02ee-4fa3-b3b9-7885f558072b",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcf1ad-8562-458b-b07c-226d1ed3dbe0",
   "metadata": {},
   "source": [
    "<!-- ![RT1 model architecture](../../imgs/rt1+.png) -->\n",
    "<center>\n",
    "    <img src=\"../imgs/rt1+.png\" alt=\"RT1 model architecture\" width=\"300\" height=\"400\">\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cea30-c012-4a53-a94b-7651270e4c38",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d547d-b935-42b3-bafa-ded2cddaec51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cbe10a4-0a68-429f-a18c-dfb4012920ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = TextEncoder(freeze=True).cuda()\n",
    "# summary(model=te, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae92bc-9841-4e53-811d-bce73315b39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1790e904-591d-4ee3-8ae3-f9ee204dbe31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# emb = te(\n",
    "#     inp_ids=sample[\"action_desc\"][\"ids\"].cuda(),\n",
    "#     mask=sample[\"action_desc\"][\"mask\"].cuda(),\n",
    "#     tok_type_ids=sample[\"action_desc\"][\"token_type_ids\"].cuda()\n",
    "# )\n",
    "\n",
    "# emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491611ae-2f70-404b-b04c-a401bad6fc56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Img Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d73ba-efef-4d32-81cb-520974830c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fe = ImageFeatureExtractor(pretrained=True, arch=\"efficientnet_b3\").cuda()\n",
    "\n",
    "# summary(fe, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9d6e5-ca31-4b20-8fe1-07412fcb4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_ftrs = fe(sample[\"in_state\"].cuda())\n",
    "\n",
    "# img_ftrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0654b94-9396-49ca-8708-5018cce9eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beea6fe0-fc93-4c1d-9f7d-ae8194f9bb29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test FiLM Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b36b8d53-a51d-4dfd-9cd6-0dabc3e1b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# film_block = FiLMBlockV2().cuda()\n",
    "# print(film_block)\n",
    "# summary(model=film_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab1a5658-8b44-4fd9-8a3e-e2c3f2380864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cond_ftrs = film_block(\n",
    "#     img_features=img_ftrs, \n",
    "#     conditioning=emb\n",
    "# )\n",
    "\n",
    "# text_cond_ftrs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e75562-714a-480f-9b03-be898c3aebd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Residual FiLM Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca31d01-12cd-4fbf-af23-1dbecfe2b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw_res = ResBlockDWConv(512, 512).cuda()\n",
    "# summary(model=dw_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44cd288-29c5-41ef-be96-72a066bfbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cond_ftrs_res = dw_res(\n",
    "#     img_features=img_ftrs, \n",
    "#     conditioning=emb\n",
    "# )\n",
    "\n",
    "# text_cond_ftrs_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00506f45-07c7-40fc-927c-3e307e91b23f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test FiLM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a904d1-b1ef-40b2-9344-f2580105ee41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# film_encoder = FiLMEncoder(\n",
    "#     arch=\"resnet34\",\n",
    "#     n_res_blocks=6,\n",
    "# ).cuda()\n",
    "\n",
    "# # print(film_encoder)\n",
    "# summary(model=film_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b70f6a7-c6b5-4743-8ecf-9cfa122dd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# out = film_encoder(\n",
    "#     x= sample[\"in_state\"].cuda(),\n",
    "#     conditioning= emb\n",
    "# )\n",
    "\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217cc28-4eb7-4f76-86ee-51dd6ae8f248",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Token Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f2da760-c2ee-4ff1-97bd-8859562a484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, C, H_W = out.shape\n",
    "# N, C, H_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb960649-ad52-4a6f-be9a-279263b0ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokL_v11 = token_learner.TokenLearnerModuleV11(feature_shape=(N, H_W, C)).cuda()\n",
    "# print(tokL_v11)\n",
    "# summary(model=tokL_v11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb43cec1-32e2-4f1d-bb5a-3ba026a57fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned_tokens = tokL_v11(out.view(N, H_W, C))\n",
    "# learned_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e045e3a-4d75-454a-9fdd-c5205f332a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3ad67e60-baf3-449f-83f7-ad5256cbcda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 512, 8])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = torch.empty((B, config.NUM_HISTORY+1, config.D_MODEL, config.NUM_LEARNED_TOKENS))\n",
    "tokenized_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce81c2-ecfd-49cf-8613-b806d585dfb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### RT-1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d56dd9db-abff-4cb9-b8af-b1b68e3e7ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b3.ra2_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b3.ra2_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Param #                   Trainable\n",
       "==============================================================================================================\n",
       "RT1Encoder                                                   --                        Partial\n",
       "├─TextEncoder: 1-1                                           --                        False\n",
       "│    └─BertModel: 2-1                                        --                        False\n",
       "│    │    └─BertEmbeddings: 3-1                              (15,891,456)              False\n",
       "│    │    └─BertEncoder: 3-2                                 (12,609,536)              False\n",
       "│    │    └─BertPooler: 3-3                                  (262,656)                 False\n",
       "│    └─Dropout: 2-2                                          --                        --\n",
       "├─FiLMEncoder: 1-2                                           --                        True\n",
       "│    └─ImageFeatureExtractor: 2-3                            --                        True\n",
       "│    │    └─EfficientNetFeatures: 3-4                        10,103,336                True\n",
       "│    │    └─VisionLanguageHead: 3-5                          197,120                   True\n",
       "│    └─ModuleList: 2-4                                       --                        True\n",
       "│    │    └─ResBlockDWConv: 3-6                              1,056,768                 True\n",
       "│    │    └─ResBlockDWConv: 3-7                              1,056,768                 True\n",
       "│    │    └─ResBlockDWConv: 3-8                              1,056,768                 True\n",
       "│    │    └─ResBlockDWConv: 3-9                              1,056,768                 True\n",
       "│    │    └─ResBlockDWConv: 3-10                             1,056,768                 True\n",
       "│    │    └─ResBlockDWConv: 3-11                             1,056,768                 True\n",
       "├─TokenLearnerModuleV11: 1-3                                 --                        True\n",
       "│    └─LayerNormalization: 2-5                               --                        True\n",
       "│    │    └─LayerNorm: 3-12                                  1,024                     True\n",
       "│    └─FeedFowardLayer: 2-6                                  --                        True\n",
       "│    │    └─Linear: 3-13                                     65,664                    True\n",
       "│    │    └─GELU: 3-14                                       --                        --\n",
       "│    │    └─Linear: 3-15                                     1,032                     True\n",
       "│    │    └─Dropout: 3-16                                    --                        --\n",
       "==============================================================================================================\n",
       "Total params: 45,472,432\n",
       "Trainable params: 16,708,784\n",
       "Non-trainable params: 28,763,648\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = RT1Encoder(\n",
    "    cnn_bacnbone=\"efficientnet_b3\", num_res_blocks=6, freeze_cnn_backbone=False\n",
    ").to(config.DEVICE)\n",
    "\n",
    "summary(model=encoder, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7405de1c-ea97-4908-aa25-7858478627c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 988 ms, sys: 1.13 s, total: 2.12 s\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512, 8]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "src_enc, tokens = encoder._encode(\n",
    "    input_ids=sample[\"action_desc\"][\"ids\"].cuda(),\n",
    "    attn_mask=sample[\"action_desc\"][\"mask\"].cuda(),\n",
    "    token_type_ids=sample[\"action_desc\"][\"token_type_ids\"].cuda(),\n",
    "    imgs=sample[\"in_state\"].cuda()\n",
    ")\n",
    "\n",
    "src_enc.shape, tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59c4bf9c-28e7-4034-815e-cdea7826c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 170 ms, total: 433 ms\n",
      "Wall time: 2.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 48, 512]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "src_enc, tokenized_inputs = encoder(\n",
    "    input_ids=sample[\"action_desc\"][\"ids\"].cuda(),\n",
    "    attn_mask=sample[\"action_desc\"][\"mask\"].cuda(),\n",
    "    token_type_ids=sample[\"action_desc\"][\"token_type_ids\"].cuda(),\n",
    "    imgs=sample[\"in_state\"].cuda()\n",
    ")\n",
    "\n",
    "tokenized_inputs.shape, tokenized_inputs.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e620f-5541-4a49-acdf-43ec4b3cce02",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "113b0f75-f651-49bb-bf86-f8a6e7ed8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = PositionalEncoder(seq_len=config.NUM_TOKENIZED_INPUTS).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b15c6175-95d5-4935-b881-a3e2d3aa99d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6933, 0.5674, 0.5251], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[0, 0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4234188a-1349-4007-998a-ef75ab9e0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 48, 512])\n"
     ]
    }
   ],
   "source": [
    "pos_enc = pos_encoding(tokenized_inputs)\n",
    "print(pos_enc.shape)\n",
    "\n",
    "learned_tokens = tokenized_inputs + pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "646c9b86-81e7-4e9d-b055-c05e16f15143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.6886, 13.8378, 11.8820], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc[0, 0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56763ee6-7e8d-4ced-a5a4-d8e39a5192ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.3819, 14.4052, 12.4072], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_tokens[0, 0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "282610d7-fecb-4a9f-b63b-2dfb7db52624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 48])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = generate_causal_attention_mask()\n",
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11506d-24e5-4b3f-b027-595874487e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce2c336-d441-4f0d-a879-e4337d2abc7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a4574-0dde-433d-8a5d-f1a76bf4c54d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cross Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "0d496107-d853-4e1b-bbfc-651fecdb28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim:int=config.D_MODEL, \n",
    "        dropout_rate:float=config.DROPOUT_RATE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Linear transformations for queries, keys, and values for input sequences\n",
    "        self.w_q_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_k_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_v_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Linear transformations for queries, keys, and values for output sequences\n",
    "        self.w_q_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_k_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_v_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_seq, output_seq):\n",
    "        \n",
    "        input_shape = input_seq.shape\n",
    "        output_shape = output_seq.shape\n",
    "\n",
    "        # Linear transformations + split into num_heads for both input and output sequences\n",
    "        q_input = self.w_q_input(input_seq)\n",
    "        k_input = self.w_k_input(input_seq)\n",
    "        v_input = self.w_v_input(input_seq)\n",
    "        \n",
    "        q_output = self.w_q_output(output_seq)\n",
    "        k_output = self.w_k_output(output_seq)\n",
    "        v_output = self.w_v_output(output_seq)\n",
    "\n",
    "        # Transpose dimensions for input and output sequences\n",
    "        q_input = q_input.transpose(1, 2)\n",
    "        k_input = k_input.transpose(1, 2)\n",
    "        v_input = v_input.transpose(1, 2)\n",
    "        q_output = q_output.transpose(1, 2)\n",
    "        k_output = k_output.transpose(1, 2)\n",
    "        v_output = v_output.transpose(1, 2)\n",
    "\n",
    "        # Calculate cross-attention scores\n",
    "        attn_scores = torch.matmul(q_output, k_input.transpose(-2, -1))  # (B, num_heads, L_output, L_input)\n",
    "        attn_scores = attn_scores / math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Softmax\n",
    "        attn_W = self._softmax(attn_scores)\n",
    "        attn_W = self.dropout(attn_W)\n",
    "\n",
    "        # Calculate cross-attention values\n",
    "        attn_values = torch.matmul(attn_W, v_input)  # (B, num_heads, L_output, embed_dim)\n",
    "\n",
    "        return attn_values, attn_W\n",
    "        \n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        \n",
    "        self.inf = 1e9\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = self.embed_dim // self.num_heads\n",
    "        \n",
    "        # Linear transformations for queries, keys, and values for input and output sequences\n",
    "        self.w_q_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_k_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_v_input = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_q_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_k_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.w_v_output = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_seq, output_seq):\n",
    "        input_shape = input_seq.shape\n",
    "        output_shape = output_seq.shape\n",
    "\n",
    "        # Linear transformations + split into num_heads for both input and output sequences\n",
    "        q_input = self.w_q_input(input_seq).view(input_shape[0], -1, self.num_heads, self.d_k)\n",
    "        k_input = self.w_k_input(input_seq).view(input_shape[0], -1, self.num_heads, self.d_k)\n",
    "        v_input = self.w_v_input(input_seq).view(input_shape[0], -1, self.num_heads, self.d_k)\n",
    "        q_output = self.w_q_output(output_seq).view(output_shape[0], -1, self.num_heads, self.d_k)\n",
    "        k_output = self.w_k_output(output_seq).view(output_shape[0], -1, self.num_heads, self.d_k)\n",
    "        v_output = self.w_v_output(output_seq).view(output_shape[0], -1, self.num_heads, self.d_k)\n",
    "\n",
    "        # Transpose dimensions for input and output sequences\n",
    "        q_input = q_input.transpose(1, 2)\n",
    "        k_input = k_input.transpose(1, 2)\n",
    "        v_input = v_input.transpose(1, 2)\n",
    "        q_output = q_output.transpose(1, 2)\n",
    "        k_output = k_output.transpose(1, 2)\n",
    "        v_output = v_output.transpose(1, 2)\n",
    "\n",
    "        # Calculate cross-attention scores\n",
    "        attn_scores = torch.matmul(q_output, k_input.transpose(-2, -1))  # (B, num_heads, L_output, L_input)\n",
    "        attn_scores = attn_scores / math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Softmax\n",
    "        attn_W = self._softmax(attn_scores)\n",
    "        attn_W = self.dropout(attn_W)\n",
    "\n",
    "        # Calculate cross-attention values\n",
    "        attn_values = torch.matmul(attn_W, v_input)  # (B, num_heads, L_output, embed_dim)\n",
    "\n",
    "        return attn_values, attn_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "42e36a9c-e3ec-4623-8999-0183764bde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.randn(3, 10, 512) \n",
    "output_seq = torch.randn(3, 12, 512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386858f0-9d60-483e-8d2e-bb67b8d4246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "cross_attention = CrossAttention(embed_dim=512, num_heads=num_heads, dropout_rate=0.1)\n",
    "\n",
    "summary(cross_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ca69d5f6-a9b8-4694-8641-e3dbdfaf6102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 12, 128]), torch.Size([3, 4, 12, 10]))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attention.eval()\n",
    "\n",
    "attention_v, attention_w = cross_attention(input_seq, output_seq)\n",
    "attention_v.shape, attention_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da189f62-b8fd-4688-83f6-870d4bcb51ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "782318f8-4fea-43eb-b30d-b6e3af2ae026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = pyplot.subplots(1, num_heads, figsize=(15, 5))\n",
    "for head_idx in range(num_heads):\n",
    "    attention_weights_example_head = attention_w[example_idx, head_idx].cpu().detach().numpy()\n",
    "    ax = axes[head_idx]\n",
    "    ax.imshow(attention_weights_example_head, cmap='viridis')\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ef82d-0772-4576-ad9f-90f9f3c1d0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd8cb6-34d5-44a3-8239-2e3d1a2eed27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9ed6e737-6829-499b-adf5-a4424f253444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model:int=config.D_MODEL, \n",
    "        nhead:int=config.N_HEADS, \n",
    "        dim_feedforward:int=config.D_FF, \n",
    "        dropout:float=config.DECODER_DROPOUT_RATE\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention()\n",
    "        \n",
    "        # Layer normalization 1\n",
    "        self.norm1 = LayerNormalization()\n",
    "        \n",
    "        self.multihead_attn = MultiHeadAttention()\n",
    "\n",
    "        # Layer normalization 2\n",
    "        self.norm2 = LayerNormalization()\n",
    "        \n",
    "        # Position-wise feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization after feed-forward\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        inp:torch.Tensor, \n",
    "        mask:torch.Tensor=None, \n",
    "        return_weights:bool=False\n",
    "    ):\n",
    "        \n",
    "        # Layer normalization 1\n",
    "        inp = self.norm1(inp)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        attn_W = None\n",
    "        self_attn_output = self.self_attn(inp, inp, inp, mask=mask)\n",
    "        if return_weights:\n",
    "            self_attn_output, attn_W = self_attn_output\n",
    "        \n",
    "        # Apply dropout and add the residual connection\n",
    "        inp = inp + self.dropout(self_attn_output)\n",
    "        \n",
    "        # Layer normalization 2\n",
    "        out = self.norm2(inp)\n",
    "        \n",
    "        # Apply FF\n",
    "        ff_out = self.dropout(self.ff(out))\n",
    "        \n",
    "        # Add the residual connection\n",
    "        inp = inp + ff_out\n",
    "\n",
    "        return inp, attn_W\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers:int=config.N_DECODER_LAYERS\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        # self.target_embedding = nn.Embedding(\n",
    "        #     num_embeddings=config.TARGET_VOCAB_SIZE, \n",
    "        #     embedding_dim=config.D_MODEL, \n",
    "        #     padding_idx=config.TARGETS_MAPPING[\"[PAD]\"]\n",
    "        # )\n",
    "        \n",
    "        # token embedding\n",
    "        self.token_embedding = nn.Linear(in_features=config.NUM_TOKENIZED_INPUTS, out_features=config.D_FF)\n",
    "               \n",
    "        # transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer()\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \n",
    "        # Glorot / fan_avg. Initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            \n",
    "    def forward(\n",
    "        self, \n",
    "        inp:torch.Tensor, \n",
    "        mask:torch.Tensor=None, \n",
    "    ):\n",
    "        \n",
    "        B, T, D = inp.shape\n",
    "        \n",
    "        # positions = torch.eye(T).repeat(B, 1, 1)\n",
    "        \n",
    "        # embed tokens\n",
    "        inp = self.token_embedding(inp)\n",
    "        # inp = inp + self.position_embedding(positions) # positions to be added \n",
    "        \n",
    "        # run attention self-attention modules\n",
    "        attn_Ws = []\n",
    "        for layer in self.layers:\n",
    "            inp, attn_W = layer(inp, mask=mask)\n",
    "            if attn_W is not None:\n",
    "                attn_Ws.append(attn_W)\n",
    "                \n",
    "        return inp, attn_Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3f321-d559-4e51-be2f-49145979574e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test MultiHeadAttention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "801a09f2-d7e8-48f7-8664-1efae4c1ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class SelfAttentionHead(nn.Module):\n",
    "#     def __init__(self, d_model:int=config.D_MODEL):\n",
    "#         super(SelfAttentionHead, self).__init__()\n",
    "#         self.inf = 1e9\n",
    "#         self.d_k = d_model\n",
    "        \n",
    "#         self.dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "#         self._softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "#         self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "#         self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "#         self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "#         self.output_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def self_attention(\n",
    "#         self, \n",
    "#         inp:toch.Tensor, \n",
    "#         mask=None, \n",
    "#         return_weights=True\n",
    "#     ):\n",
    "#         # Linear calculation\n",
    "#         q = self.w_q(inp)\n",
    "#         k = self.w_k(inp)\n",
    "#         v = self.w_v(inp)\n",
    "\n",
    "#         # Scaled Dot-Product Attention\n",
    "#         matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "#         scaled_attention_logits = matmul_qk / math.sqrt(self.d_k)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             scaled_attention_logits += (mask * -self.inf)\n",
    "\n",
    "#         attention_weights = self._softmax(scaled_attention_logits)\n",
    "#         attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "#         output = torch.matmul(attention_weights, v)\n",
    "\n",
    "#         if return_weights:\n",
    "#             return output, attention_weights\n",
    "#         else:\n",
    "#             return output, None\n",
    "\n",
    "#     def forward(\n",
    "#         self, \n",
    "#         inp:torch.Tensor, \n",
    "#         mask=None, \n",
    "#         return_weights=False\n",
    "#     ):\n",
    "        \n",
    "#         attn_values, attn_W = self.self_attention(\n",
    "#             inp, \n",
    "#             mask=mask, \n",
    "#             return_weights=return_weights\n",
    "#         )\n",
    "        \n",
    "#         return attn_values, attn_W\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         d_model:int=config.D_MODEL, \n",
    "#         num_heads:int=config.N_HEADS\n",
    "#     ):\n",
    "#         super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "\n",
    "#         self.attention_heads = nn.ModuleList([SelfAttentionHead(d_model) for _ in range(num_heads)])\n",
    "\n",
    "#     def forward(\n",
    "#         self, \n",
    "#         inp:torch.Tensor,  \n",
    "#         mask=None, \n",
    "#         return_weights=True\n",
    "#     ):\n",
    "#         B = inp.shape[0]\n",
    "        \n",
    "#         head_outputs = [head(inp, mask=mask, return_weights=return_weights) for head in self.attention_heads]\n",
    "\n",
    "#         # Combine the results from different heads\n",
    "#         combined_output = torch.cat(\n",
    "#             [output[0].view(B, -1, self.num_heads, self.head_dim) for output in head_outputs], \n",
    "#             dim=-1\n",
    "#         )\n",
    "#         attention_weights = torch.stack(\n",
    "#             [output[1] for output in head_outputs], \n",
    "#             dim=1\n",
    "#         )\n",
    "\n",
    "#         return combined_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e1ff1f4-1702-476f-bc0a-0e9c73b7c3d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadSelfAttention(\n",
      "  (attention_heads): ModuleList(\n",
      "    (0-7): 8 x SelfAttentionHead(\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "      (_softmax): Softmax(dim=-1)\n",
      "      (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiHeadSelfAttention                   --\n",
       "├─ModuleList: 1-1                        --\n",
       "│    └─SelfAttentionHead: 2-1            --\n",
       "│    │    └─Dropout: 3-1                 --\n",
       "│    │    └─Softmax: 3-2                 --\n",
       "│    │    └─Linear: 3-3                  262,144\n",
       "│    │    └─Linear: 3-4                  262,144\n",
       "│    │    └─Linear: 3-5                  262,144\n",
       "│    │    └─Linear: 3-6                  262,656\n",
       "│    └─SelfAttentionHead: 2-2            --\n",
       "│    │    └─Dropout: 3-7                 --\n",
       "│    │    └─Softmax: 3-8                 --\n",
       "│    │    └─Linear: 3-9                  262,144\n",
       "│    │    └─Linear: 3-10                 262,144\n",
       "│    │    └─Linear: 3-11                 262,144\n",
       "│    │    └─Linear: 3-12                 262,656\n",
       "│    └─SelfAttentionHead: 2-3            --\n",
       "│    │    └─Dropout: 3-13                --\n",
       "│    │    └─Softmax: 3-14                --\n",
       "│    │    └─Linear: 3-15                 262,144\n",
       "│    │    └─Linear: 3-16                 262,144\n",
       "│    │    └─Linear: 3-17                 262,144\n",
       "│    │    └─Linear: 3-18                 262,656\n",
       "│    └─SelfAttentionHead: 2-4            --\n",
       "│    │    └─Dropout: 3-19                --\n",
       "│    │    └─Softmax: 3-20                --\n",
       "│    │    └─Linear: 3-21                 262,144\n",
       "│    │    └─Linear: 3-22                 262,144\n",
       "│    │    └─Linear: 3-23                 262,144\n",
       "│    │    └─Linear: 3-24                 262,656\n",
       "│    └─SelfAttentionHead: 2-5            --\n",
       "│    │    └─Dropout: 3-25                --\n",
       "│    │    └─Softmax: 3-26                --\n",
       "│    │    └─Linear: 3-27                 262,144\n",
       "│    │    └─Linear: 3-28                 262,144\n",
       "│    │    └─Linear: 3-29                 262,144\n",
       "│    │    └─Linear: 3-30                 262,656\n",
       "│    └─SelfAttentionHead: 2-6            --\n",
       "│    │    └─Dropout: 3-31                --\n",
       "│    │    └─Softmax: 3-32                --\n",
       "│    │    └─Linear: 3-33                 262,144\n",
       "│    │    └─Linear: 3-34                 262,144\n",
       "│    │    └─Linear: 3-35                 262,144\n",
       "│    │    └─Linear: 3-36                 262,656\n",
       "│    └─SelfAttentionHead: 2-7            --\n",
       "│    │    └─Dropout: 3-37                --\n",
       "│    │    └─Softmax: 3-38                --\n",
       "│    │    └─Linear: 3-39                 262,144\n",
       "│    │    └─Linear: 3-40                 262,144\n",
       "│    │    └─Linear: 3-41                 262,144\n",
       "│    │    └─Linear: 3-42                 262,656\n",
       "│    └─SelfAttentionHead: 2-8            --\n",
       "│    │    └─Dropout: 3-43                --\n",
       "│    │    └─Softmax: 3-44                --\n",
       "│    │    └─Linear: 3-45                 262,144\n",
       "│    │    └─Linear: 3-46                 262,144\n",
       "│    │    └─Linear: 3-47                 262,144\n",
       "│    │    └─Linear: 3-48                 262,656\n",
       "=================================================================\n",
       "Total params: 8,392,704\n",
       "Trainable params: 8,392,704\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadSelfAttention().cuda()\n",
    "print(mha)\n",
    "summary(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04796d95-349d-4d81-817a-7f23d8f15541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 48, 8, 512]), torch.Size([4, 8, 48, 48]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem, attn_w = mha(inp=learned_tokens, return_weights=True)\n",
    "\n",
    "mem.shape, attn_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90db1a-ba0d-44c1-bd01-09f08133fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_self_attention(attn_w)\n",
    "plot_self_attention(attn_w, example_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd136fd2-05dc-4569-8f29-626562bc10f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a8a60-29d5-4103-8083-a83daa383ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "210efe9b-24f9-4e1a-a115-29347d9849eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a5ebf21e-d456-4f85-bb0c-fd1056c517f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TransformerDecoder                       --\n",
       "├─Linear: 1-1                            25,088\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─TransformerDecoderLayer: 2-1      --\n",
       "│    │    └─MultiHeadAttention: 3-1      1,049,088\n",
       "│    │    └─LayerNormalization: 3-2      1,024\n",
       "│    │    └─MultiHeadAttention: 3-3      1,049,088\n",
       "│    │    └─LayerNormalization: 3-4      1,024\n",
       "│    │    └─Sequential: 3-5              525,312\n",
       "│    │    └─LayerNorm: 3-6               1,024\n",
       "│    │    └─Dropout: 3-7                 --\n",
       "│    └─TransformerDecoderLayer: 2-2      --\n",
       "│    │    └─MultiHeadAttention: 3-8      1,049,088\n",
       "│    │    └─LayerNormalization: 3-9      1,024\n",
       "│    │    └─MultiHeadAttention: 3-10     1,049,088\n",
       "│    │    └─LayerNormalization: 3-11     1,024\n",
       "│    │    └─Sequential: 3-12             525,312\n",
       "│    │    └─LayerNorm: 3-13              1,024\n",
       "│    │    └─Dropout: 3-14                --\n",
       "│    └─TransformerDecoderLayer: 2-3      --\n",
       "│    │    └─MultiHeadAttention: 3-15     1,049,088\n",
       "│    │    └─LayerNormalization: 3-16     1,024\n",
       "│    │    └─MultiHeadAttention: 3-17     1,049,088\n",
       "│    │    └─LayerNormalization: 3-18     1,024\n",
       "│    │    └─Sequential: 3-19             525,312\n",
       "│    │    └─LayerNorm: 3-20              1,024\n",
       "│    │    └─Dropout: 3-21                --\n",
       "│    └─TransformerDecoderLayer: 2-4      --\n",
       "│    │    └─MultiHeadAttention: 3-22     1,049,088\n",
       "│    │    └─LayerNormalization: 3-23     1,024\n",
       "│    │    └─MultiHeadAttention: 3-24     1,049,088\n",
       "│    │    └─LayerNormalization: 3-25     1,024\n",
       "│    │    └─Sequential: 3-26             525,312\n",
       "│    │    └─LayerNorm: 3-27              1,024\n",
       "│    │    └─Dropout: 3-28                --\n",
       "│    └─TransformerDecoderLayer: 2-5      --\n",
       "│    │    └─MultiHeadAttention: 3-29     1,049,088\n",
       "│    │    └─LayerNormalization: 3-30     1,024\n",
       "│    │    └─MultiHeadAttention: 3-31     1,049,088\n",
       "│    │    └─LayerNormalization: 3-32     1,024\n",
       "│    │    └─Sequential: 3-33             525,312\n",
       "│    │    └─LayerNorm: 3-34              1,024\n",
       "│    │    └─Dropout: 3-35                --\n",
       "│    └─TransformerDecoderLayer: 2-6      --\n",
       "│    │    └─MultiHeadAttention: 3-36     1,049,088\n",
       "│    │    └─LayerNormalization: 3-37     1,024\n",
       "│    │    └─MultiHeadAttention: 3-38     1,049,088\n",
       "│    │    └─LayerNormalization: 3-39     1,024\n",
       "│    │    └─Sequential: 3-40             525,312\n",
       "│    │    └─LayerNorm: 3-41              1,024\n",
       "│    │    └─Dropout: 3-42                --\n",
       "=================================================================\n",
       "Total params: 15,784,448\n",
       "Trainable params: 15,784,448\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = TransformerDecoder()\n",
    "summary(model=dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab8990-184b-4854-980e-ef4887300b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out = dec(\n",
    "    src=src_enc, \n",
    "    encoder_outputs=tokens,\n",
    "    y=None #sample[\"motor_cmd\"][\"ids\"].cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f252ad6-5c3a-44ce-ab3d-4d4d75d1960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e365900-13ff-4dd8-b789-6f45b15c0564",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Action Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "929f0bc4-3be4-4b84-a52a-7235e9af947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model:int=config.D_MODEL, \n",
    "        vocab_size:int=len(config.TARGETS),\n",
    "        action_bins:int=config.ACTION_BINS,\n",
    "        num_actons:int=config.NUM_ACTION_SLOTS\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # attrs\n",
    "        self.action_bins = action_bins\n",
    "        \n",
    "        # layers\n",
    "        self.norm = LayerNormalization()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "        self._softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        out = self.norm(tokens)\n",
    "        out = self.proj(out)\n",
    "        out = self._softmax(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a9fef7ba-432f-448c-bac6-c3651e1f1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionGenerator(\n",
      "  (norm): LayerNormalization(\n",
      "    (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (proj): Linear(in_features=512, out_features=52, bias=True)\n",
      "  (_softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ActionGenerator                          --\n",
       "├─LayerNormalization: 1-1                --\n",
       "│    └─LayerNorm: 2-1                    1,024\n",
       "├─Linear: 1-2                            26,676\n",
       "├─LogSoftmax: 1-3                        --\n",
       "=================================================================\n",
       "Total params: 27,700\n",
       "Trainable params: 27,700\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = ActionGenerator().cuda()\n",
    "print(generator)\n",
    "summary(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013f117-e5e3-4b2c-b14b-d2260e40f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, T = tokens.shape\n",
    "\n",
    "N, C, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a76ea-ae3b-403a-b8e2-a817f50498fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = generator(tokens.view(N, T, C))\n",
    "\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14293b-b031-477b-a643-3811927b1bb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RT-1 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ec749-1fbf-4072-b191-b6e3ad2f243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.transformer = TransformerDecoder()\n",
    "        self.action_generator = ActionGenerator()\n",
    "        \n",
    "    def _decode_predictions(self, preds, method:str=\"greedy\"):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                preds: predictions (logits)\n",
    "                method: decoding strategy. one of [\"greedy\", \"beam-search\"]\n",
    "                \n",
    "            Returns:\n",
    "                actions: decoded predictions as sequence of actions.\n",
    "            \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, learned_tokens):\n",
    "        \n",
    "        # positional encoding\n",
    "        pos_enc = self.positional_encoder(imgs)\n",
    "        \n",
    "        # Generate attention mask\n",
    "        attention_mask = generate_causal_attention_mask()\n",
    "        # Add positional encoding\n",
    "        pos_enc = pos_encoding(learned_tokens)\n",
    "        # print(pos_enc.shape)\n",
    "        learned_tokens = learned_tokens + pos_enc\n",
    "        # Attend to learned tokens\n",
    "        attended_tokens = self.transformer(learned_tokens, mask = ~attention_mask)\n",
    "        out = reduce(attended_tokens, 'b (f n) d -> b f d', 'mean', f = (config.NUM_HISTORY + 1))\n",
    "        \n",
    "        # generate outputs\n",
    "        logits = self.action_generator(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f893fa-9324-4244-aad2-a66db2243700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cde5f94-8a3c-458d-94da-9655d7ddb2d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RT-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033aac5-7312-47d8-8ff8-8044ebb2fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = RT1Encder()\n",
    "        self.decoder = RT1Decoder()\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids, imgs):\n",
    "        \n",
    "        tokens = self.encode(input_ids, attn_mask, token_type_ids, imgs)\n",
    "        out = self.decode(tokens, )\n",
    "    \n",
    "    def encode(self, input_ids, attn_mask, token_type_ids, imgs):\n",
    "        return self.encoder(input_ids, attn_mask, token_type_ids, imgs)\n",
    "    \n",
    "    def decode(self, enc_outputs, x_mask, y, y_mask):\n",
    "        return self.decoder(y, enc_outputs, src_mask, tgt_mask)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd461-7028-49de-bcb9-e8464433f548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a346bbe-d174-40a9-a807-e2cd63665ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bebcb-a94a-4272-b68d-5eeee9dbd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMF-BE",
   "language": "python",
   "name": "smf-be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
