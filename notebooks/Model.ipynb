{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11449575-6314-4708-9193-b279891e836c",
   "metadata": {},
   "source": [
    "## Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f302c0d-40f1-4bac-bf57-df54c89e72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 23 05:23:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    39W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ee382-8c34-4130-99d3-80d5abdfbf1a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d150586-d672-4999-9a7f-743a7d0d3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2664559-8789-4785-83d6-26327fe5ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpvphysszz\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpvphysszz/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from einops import pack, unpack, repeat, reduce, rearrange\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModel, AdamW, AutoConfig, get_linear_schedule_with_warmup)\n",
    "\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09a35d4-17a4-437a-ad75-cfdbf9215827",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ef863cc-4128-49a3-a156-fbdc859a9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataloader import BEDataset, BEDataModule\n",
    "from transformer import PositionalEncoder, TransformerDecoder, LayerNormalization, FeedFowardLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b26cc-b5a8-4a5a-9e6c-6d2721d34c58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e68541d-e1c2-48df-a18d-c37c9da42002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ID</th>\n",
       "      <th>in_state</th>\n",
       "      <th>goal_state</th>\n",
       "      <th>action_description</th>\n",
       "      <th>motor_cmd</th>\n",
       "      <th>len_action_desc</th>\n",
       "      <th>len_motor_cmd</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the fork to the right of buttermilk</td>\n",
       "      <td>:FORK GREEN POSE-9 :BUTTERMILK GREEN POSE-2 :F...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>move the bottle backwards</td>\n",
       "      <td>:BOTTLE RED POSE-2 :BOTTLE  #'*backward-transf...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4235</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the bottle to the left of breakfast-cereal</td>\n",
       "      <td>:BOTTLE RED POSE-7 :BREAKFAST-CEREAL BLUE POSE...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6990</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the milk in front of bottle</td>\n",
       "      <td>:MILK BLUE POSE-8 :BOTTLE RED POSE-4 :MILK  #'...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7096</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the cup in front of glasses</td>\n",
       "      <td>:CUP GREEN POSE-6 :GLASSES RED POSE-2 :CUP  #'...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_ID  in_state  goal_state  \\\n",
       "0       7294         0          10   \n",
       "1        405         0           8   \n",
       "2       4235         0          10   \n",
       "3       6990         0          10   \n",
       "4       7096         0          10   \n",
       "\n",
       "                               action_description  \\\n",
       "0         put the fork to the right of buttermilk   \n",
       "1                       move the bottle backwards   \n",
       "2  put the bottle to the left of breakfast-cereal   \n",
       "3                 put the milk in front of bottle   \n",
       "4                 put the cup in front of glasses   \n",
       "\n",
       "                                           motor_cmd  len_action_desc  \\\n",
       "0  :FORK GREEN POSE-9 :BUTTERMILK GREEN POSE-2 :F...                8   \n",
       "1  :BOTTLE RED POSE-2 :BOTTLE  #'*backward-transf...                4   \n",
       "2  :BOTTLE RED POSE-7 :BREAKFAST-CEREAL BLUE POSE...                8   \n",
       "3  :MILK BLUE POSE-8 :BOTTLE RED POSE-4 :MILK  #'...                7   \n",
       "4  :CUP GREEN POSE-6 :GLASSES RED POSE-2 :CUP  #'...                7   \n",
       "\n",
       "   len_motor_cmd version  \n",
       "0             11      v2  \n",
       "1              8      v1  \n",
       "2             11      v2  \n",
       "3             11      v2  \n",
       "4             11      v2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv(os.path.join(config.DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7cd7ee-95e6-450a-b7bf-35a626daf357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building data object\n",
    "ds = BEDataset(\n",
    "    df=csv    \n",
    ")\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ccf421-7092-4d19-b7a8-84d4864f34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  4876\n",
      "====================================================================================================\n",
      "ID\t:  9401\n",
      ">> InState\t:  torch.Size([3, 224, 224])\n",
      ">> Desc\t:\n",
      "{'ids': tensor([  101,  2404,  1996,  5442,  2000,  1996,  2157,  1997, 12256, 17130,\n",
      "         2378,   102,     0,     0,     0,     0]),\n",
      " 'length': 8,\n",
      " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]),\n",
      " 'raw': 'put the knife to the right of mondamin',\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      ">> Cmd\t:\n",
      "{'ids': tensor([ 0, 17, 41, 30, 18, 41, 36, 17, 48, 18,  2,  2,  2,  2,  2,  2]),\n",
      " 'length': 11,\n",
      " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n",
      " 'raw': ':KNIFE GREEN POSE-10 :MONDAMIN GREEN POSE-3 :KNIFE  '\n",
      "        \"#'*rightward-transformation*  :MONDAMIN\"}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# fetching example\n",
    "rand_idx = np.random.randint(low=0, high=len(ds))\n",
    "ex = ds[rand_idx]\n",
    "\n",
    "print(\"Dataset size: \", len(ds))\n",
    "print(\"=\"*100)\n",
    "print(\"ID\\t: \", ex[\"sample_id\"])\n",
    "print(\">> InState\\t: \", ex[\"in_state\"].shape)\n",
    "print(\">> Desc\\t:\")\n",
    "pprint(ex[\"action_desc\"])\n",
    "print(\">> Cmd\\t:\")\n",
    "pprint(ex[\"motor_cmd\"])\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425b526-43d7-483b-b3bc-02028bfb4b7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820bb68f-3341-4d6e-9cea-80dbdf2de002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training on 3610 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # examples: 4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Validating on 1266 samples.\n"
     ]
    }
   ],
   "source": [
    "dm = BEDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383eaa10-47d7-4620-928e-8fbcd6889d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      ">> train data loader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "# train batches\t: 451\n",
      "In \t\t\t:  torch.Size([8, 3, 224, 224])\n",
      "Action desc \t\t:  torch.Size([8, 16])\n",
      "Action desc (len) \t:  torch.Size([8])\n",
      "CMD \t\t\t:  torch.Size([8, 16])\n",
      "CMD(len) \t\t:  torch.Size([8])\n",
      "\n",
      "IDs & decided tokens\n",
      "[101, 2404, 1996, 5442, 2000, 1996, 2157, 1997, 5442, 102, 0, 0, 0, 0, 0, 0]\n",
      "put the knife to the right of knife\n",
      "\n",
      "[0, 17, 41, 28, 17, 31, 40, 17, 48, 17, 2, 2, 2, 2, 2, 2]\n",
      ":KNIFE GREEN POSE-8 :KNIFE RED POSE-2 :KNIFE #'*rightward-transformation* :KNIFE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "logging.info(\"\\n>> train data loader\")\n",
    "print(f\"# train batches\\t: {len(dm.train_dataloader())}\")\n",
    "for data in dm.train_dataloader():\n",
    "    # pprint(data)\n",
    "    sample_id, in_state, ad, cmd = data[\"sample_id\"], data[\"in_state\"], data[\"action_desc\"], data[\"motor_cmd\"]\n",
    "    print(\"In \\t\\t\\t: \", in_state.shape)\n",
    "    print(\"Action desc \\t\\t: \", ad[\"ids\"].shape)\n",
    "    print(\"Action desc (len) \\t: \", ad[\"length\"].shape)\n",
    "    print(\"CMD \\t\\t\\t: \", cmd[\"ids\"].shape)\n",
    "    print(\"CMD(len) \\t\\t: \", cmd[\"length\"].shape)\n",
    "    break\n",
    "\n",
    "print(\"\\nIDs & decided tokens\")\n",
    "for data in dm.train_dataloader():\n",
    "    print(data[\"action_desc\"][\"ids\"][0].tolist())\n",
    "    print(dm.train_ds._decode_inputs(data[\"action_desc\"][\"ids\"][0].tolist()))\n",
    "    print()\n",
    "    print(data[\"motor_cmd\"][\"ids\"][0].tolist())\n",
    "    print(dm.train_ds._decode_outputs(data[\"motor_cmd\"][\"ids\"][0].tolist()))\n",
    "\n",
    "    break\n",
    "    \n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b32ff7-02ee-4fa3-b3b9-7885f558072b",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcf1ad-8562-458b-b07c-226d1ed3dbe0",
   "metadata": {},
   "source": [
    "<!-- ![RT1 model architecture](../../imgs/rt1+.png) -->\n",
    "<center>\n",
    "    <img src=\"../imgs/rt1+.png\" alt=\"RT1 model architecture\" width=\"500\" height=\"300\">\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cea30-c012-4a53-a94b-7651270e4c38",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd8cfa-4153-48e7-bbad-c11ba0c6357d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be96cc06-3693-4e52-8000-23a28efc0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(model:nn.Module):\n",
    "    return nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "def conv(ic, oc, k, s, p, activation:str=\"GELU\"):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ic, oc, k, s, p),\n",
    "        getattr(nn, activation)(),\n",
    "        nn.BatchNorm2d(oc),\n",
    "    )\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained:bool=True, \n",
    "        arch:str=\"resnet34\",\n",
    "        freeze:bool=True\n",
    "    ):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.freeze = freeze\n",
    "\n",
    "        if self.pretrained:\n",
    "            self.arch   = getattr(models, arch)(weights=\"IMAGENET1K_V1\")\n",
    "            self.fe     = get_backbone(model=self.arch)\n",
    "        else:\n",
    "            self.fe = nn.Sequential(\n",
    "            conv(3, 128, 5, 2, 2),\n",
    "            conv(128, 128, 3, 2, 1),\n",
    "            conv(128, 128, 3, 2, 1),\n",
    "            conv(128, 128, 3, 1, 1),\n",
    "            conv(128, 128, 3, 1, 1),\n",
    "        )\n",
    "            \n",
    "        if self.freeze:\n",
    "            self._freeze_model()\n",
    "            \n",
    "    def _freeze_model(self):\n",
    "        for param in self.fe.parameters():\n",
    "            param.requires_grad = False \n",
    "\n",
    "    def forward(self, x, flat_out:bool=False):\n",
    "        if self.pretrained:\n",
    "            enc = self.fe(x)\n",
    "            if flat_out:\n",
    "                return torch.flatten(enc, 1)\n",
    "            else:\n",
    "                return enc\n",
    "        else:\n",
    "            return self.fe(x)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_channels, n_classes):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(prev_channels, 512, 1, 1, 0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, n_classes))\n",
    "\n",
    "    def forward(self, x, return_feats:bool=True):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        feats = self.global_max_pool(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return torch.flatten(feats, 1)\n",
    "        else:\n",
    "            x = feats.view(feats.size(0), feats.size(1))\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55d73ba-efef-4d32-81cb-520974830c74",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Param #                   Trainable\n",
       "==============================================================================================================\n",
       "FeatureExtractor                                             --                        Partial\n",
       "├─EfficientNet: 1-1                                          --                        Partial\n",
       "│    └─Sequential: 2-1                                       --                        False\n",
       "│    │    └─Conv2dNormActivation: 3-1                        (1,160)                   False\n",
       "│    │    └─Sequential: 3-2                                  (3,504)                   False\n",
       "│    │    └─Sequential: 3-3                                  (48,118)                  False\n",
       "│    │    └─Sequential: 3-4                                  (110,912)                 False\n",
       "│    │    └─Sequential: 3-5                                  (638,700)                 False\n",
       "│    │    └─Sequential: 3-6                                  (1,387,760)               False\n",
       "│    │    └─Sequential: 3-7                                  (4,628,964)               False\n",
       "│    │    └─Sequential: 3-8                                  (3,284,218)               False\n",
       "│    │    └─Conv2dNormActivation: 3-9                        (592,896)                 False\n",
       "│    └─AdaptiveAvgPool2d: 2-2                                --                        --\n",
       "│    └─Sequential: 2-3                                       --                        True\n",
       "│    │    └─Dropout: 3-10                                    --                        --\n",
       "│    │    └─Linear: 3-11                                     1,537,000                 True\n",
       "├─Sequential: 1-2                                            10,696,232                False\n",
       "│    └─Sequential: 2-4                                       (recursive)               False\n",
       "│    │    └─Conv2dNormActivation: 3-12                       (recursive)               False\n",
       "│    │    └─Sequential: 3-13                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-14                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-15                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-16                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-17                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-18                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-19                                 (recursive)               False\n",
       "│    │    └─Conv2dNormActivation: 3-20                       (recursive)               False\n",
       "==============================================================================================================\n",
       "Total params: 22,929,464\n",
       "Trainable params: 1,537,000\n",
       "Non-trainable params: 21,392,464\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = FeatureExtractor(pretrained=True, arch=\"efficientnet_b3\").cuda()\n",
    "\n",
    "summary(fe, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e9d6e5-ca31-4b20-8fe1-07412fcb4f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536, 7, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ftrs = fe(ex[\"in_state\"].unsqueeze(0).cuda())\n",
    "\n",
    "img_ftrs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88aa1a-8e3a-4abf-998d-edda663b0a55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Film Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13693bd3-cfb6-4f7d-be30-5fe251079c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FiLMBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        beta = beta.view(x.size(0), x.size(1), 1, 1)\n",
    "        gamma = gamma.view(x.size(0), x.size(1), 1, 1)\n",
    "\n",
    "        x = gamma * x + beta\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c64a2-4b72-4cfe-a973-9b9f22932605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Film Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1990061-09b1-42fe-84aa-ed92c4b880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_place, out_place):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_place, out_place, 1, 1, 0)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_place, out_place, 3, 1, 1)\n",
    "        self.norm2 = nn.BatchNorm2d(out_place)\n",
    "        self.film = FiLMBlock()\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, beta, gamma):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.film(x, beta, gamma)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d547d-b935-42b3-bafa-ded2cddaec51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6505cc1-9649-490b-922c-d31deda161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, dropout_rate:float=config.TEXT_ENC_DROPOUT, freeze:bool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze = freeze\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(config.LANG_MODEL_NAME)\n",
    "        self.text_encoder = AutoModel.from_pretrained(config.LANG_MODEL_NAME, config=model_config)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        if self.freeze:\n",
    "            self._freeze_model()\n",
    "\n",
    "    def _freeze_model(self):\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False \n",
    "        \n",
    "    def forward(self, inp_ids, mask, tok_type_ids):\n",
    "        # embed NL instructions\n",
    "        text_enc = self.text_encoder(\n",
    "            input_ids=inp_ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=tok_type_ids\n",
    "        ).pooler_output\n",
    "        \n",
    "        # print(text_enc.shape)\n",
    "        text_enc = self.dropout(text_enc)\n",
    "        \n",
    "        return text_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cbe10a4-0a68-429f-a18c-dfb4012920ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Param #                   Trainable\n",
       "=========================================================================================================\n",
       "TextEncoder                                             --                        False\n",
       "├─BertModel: 1-1                                        --                        False\n",
       "│    └─BertEmbeddings: 2-1                              --                        False\n",
       "│    │    └─Embedding: 3-1                              (7,813,632)               False\n",
       "│    │    └─Embedding: 3-2                              (131,072)                 False\n",
       "│    │    └─Embedding: 3-3                              (512)                     False\n",
       "│    │    └─LayerNorm: 3-4                              (512)                     False\n",
       "│    │    └─Dropout: 3-5                                --                        --\n",
       "│    └─BertEncoder: 2-2                                 --                        False\n",
       "│    │    └─ModuleList: 3-6                             (3,159,040)               False\n",
       "│    └─BertPooler: 2-3                                  --                        False\n",
       "│    │    └─Linear: 3-7                                 (65,792)                  False\n",
       "│    │    └─Tanh: 3-8                                   --                        --\n",
       "├─Dropout: 1-2                                          --                        --\n",
       "=========================================================================================================\n",
       "Total params: 11,170,560\n",
       "Trainable params: 0\n",
       "Non-trainable params: 11,170,560\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextEncoder(freeze=True).cuda()\n",
    "te._freeze_model()\n",
    "summary(model=te, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c58d15f0-b9f5-4a37-8dc4-2eb768d42ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': 'put the knife to the right of mondamin',\n",
       " 'ids': tensor([  101,  2404,  1996,  5442,  2000,  1996,  2157,  1997, 12256, 17130,\n",
       "          2378,   102,     0,     0,     0,     0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'length': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[\"action_desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1790e904-591d-4ee3-8ae3-f9ee204dbe31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb = te(\n",
    "    inp_ids=ex[\"action_desc\"][\"ids\"].unsqueeze(0).cuda(),\n",
    "    mask=ex[\"action_desc\"][\"mask\"].unsqueeze(0).cuda(),\n",
    "    tok_type_ids=ex[\"action_desc\"][\"token_type_ids\"].unsqueeze(0).cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50303d89-95ce-430a-8e74-5df3de3d8349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9761f-f182-4918-81c1-24aac2cc73e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Film Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6cfec091-c164-480e-b96a-76af494e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_res_blocks:int=3,\n",
    "        out_dim:int=256,\n",
    "        n_channels:int=512,\n",
    "        dim_description:int=32,\n",
    "        arch:str=\"resnet18\"\n",
    "    ):\n",
    "        super(FiLMEncoder, self).__init__()\n",
    "        \n",
    "        if arch in [\"resnet18\", \"resnet34\"]:\n",
    "            n_channels = 512\n",
    "        # elif \"resnet\" in arch:\n",
    "        #     n_channels = 2048\n",
    "        elif \"convnext\" in arch:\n",
    "            n_channels = 768\n",
    "\n",
    "        self.dim_description = dim_description\n",
    "        self.film_generator = nn.Linear(self.dim_description, 2 * n_res_blocks * n_channels)\n",
    "        self.feature_extractor = FeatureExtractor(arch=arch)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_res_blocks):\n",
    "            self.res_blocks.append(ResBlock(n_channels + 2, n_channels))\n",
    "\n",
    "        # self.head = Head(n_channels, out_dim)\n",
    "\n",
    "        self.n_res_blocks = n_res_blocks\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "    def forward(self, x, description):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.feature_extractor(x)\n",
    "        film_vector = self.film_generator(description).view(\n",
    "            batch_size, self.n_res_blocks, 2, self.n_channels)\n",
    "        \n",
    "        d = x.size(-1)\n",
    "        coordinate = torch.arange(-1, 1 + 0.00001, 2 / (d-1)).to(config.DEVICE)\n",
    "        coordinate_x = coordinate.expand(batch_size, 1, d, d)\n",
    "        coordinate_y = coordinate.view(d, 1).expand(batch_size, 1, d, d)\n",
    "        # print(f\"x.shape: {x.shape} - coordinate_x.shape: {coordinate_x.shape} - coordinate_y: {coordinate_y.shape}\")\n",
    "\n",
    "        for i, res_block in enumerate(self.res_blocks):\n",
    "            beta = film_vector[:, i, 0, :]\n",
    "            gamma = film_vector[:, i, 1, :]\n",
    "\n",
    "            x = torch.cat([x, coordinate_x, coordinate_y], 1)\n",
    "            x = res_block(x, beta, gamma)\n",
    "        \n",
    "        print(\"pre-classifier: \", x.shape)\n",
    "        feats = x #self.head(x, return_feats=False)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00506f45-07c7-40fc-927c-3e307e91b23f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test FiLM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09a904d1-b1ef-40b2-9344-f2580105ee41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiLMEncoder(\n",
      "  (film_generator): Linear(in_features=256, out_features=2048, bias=True)\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (arch): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (fe): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (res_blocks): ModuleList(\n",
      "    (0-1): 2 x ResBlock(\n",
      "      (conv1): Conv2d(514, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (film): FiLMBlock()\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "FiLMEncoder                                        --\n",
       "├─Linear: 1-1                                      526,336\n",
       "├─FeatureExtractor: 1-2                            --\n",
       "│    └─ResNet: 2-1                                 --\n",
       "│    │    └─Conv2d: 3-1                            (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       (128)\n",
       "│    │    └─ReLU: 3-3                              --\n",
       "│    │    └─MaxPool2d: 3-4                         --\n",
       "│    │    └─Sequential: 3-5                        (147,968)\n",
       "│    │    └─Sequential: 3-6                        (525,568)\n",
       "│    │    └─Sequential: 3-7                        (2,099,712)\n",
       "│    │    └─Sequential: 3-8                        (8,393,728)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 --\n",
       "│    │    └─Linear: 3-10                           513,000\n",
       "│    └─Sequential: 2-2                             11,176,512\n",
       "│    │    └─Conv2d: 3-11                           (recursive)\n",
       "│    │    └─BatchNorm2d: 3-12                      (recursive)\n",
       "│    │    └─ReLU: 3-13                             --\n",
       "│    │    └─MaxPool2d: 3-14                        --\n",
       "│    │    └─Sequential: 3-15                       (recursive)\n",
       "│    │    └─Sequential: 3-16                       (recursive)\n",
       "│    │    └─Sequential: 3-17                       (recursive)\n",
       "│    │    └─Sequential: 3-18                       (recursive)\n",
       "├─ModuleList: 1-3                                  --\n",
       "│    └─ResBlock: 2-3                               --\n",
       "│    │    └─Conv2d: 3-19                           263,680\n",
       "│    │    └─ReLU: 3-20                             --\n",
       "│    │    └─Conv2d: 3-21                           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-22                      1,024\n",
       "│    │    └─FiLMBlock: 3-23                        --\n",
       "│    │    └─ReLU: 3-24                             --\n",
       "│    └─ResBlock: 2-4                               --\n",
       "│    │    └─Conv2d: 3-25                           263,680\n",
       "│    │    └─ReLU: 3-26                             --\n",
       "│    │    └─Conv2d: 3-27                           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-28                      1,024\n",
       "│    │    └─FiLMBlock: 3-29                        --\n",
       "│    │    └─ReLU: 3-30                             --\n",
       "===========================================================================\n",
       "Total params: 28,641,384\n",
       "Trainable params: 6,288,360\n",
       "Non-trainable params: 22,353,024\n",
       "==========================================================================="
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_encoder = FiLMEncoder(\n",
    "    arch=\"resnet18\",\n",
    "    n_res_blocks=2,\n",
    "    dim_description=256 # (emb) action_description_size\n",
    ").cuda()\n",
    "\n",
    "print(film_encoder)\n",
    "summary(model=film_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1864bbea-f299-4118-a75e-4848d3c30112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 ms, sys: 137 ms, total: 151 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample = next(iter(dm.train_dataloader()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c4551cb-7bbe-4a7f-906e-9f99ea7c0e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"in_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b70f6a7-c6b5-4743-8ecf-9cfa122dd7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 2, 2, 512]' is invalid for input of size 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[63], line 39\u001b[0m, in \u001b[0;36mFiLMEncoder.forward\u001b[0;34m(self, x, description)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(x)\n\u001b[0;32m---> 39\u001b[0m film_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilm_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_res_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m coordinate \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.00001\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (d\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 2, 2, 512]' is invalid for input of size 2048"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "out = film_encoder(\n",
    "    x= sample[\"in_state\"].cuda(),\n",
    "    description= emb\n",
    ")\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ed299c6-20c5-4974-b7e5-b4a4957f20b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229376"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6aaceb0-0f2b-487e-9127-c60c7cfe4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Conv2d: 1-1                            513\n",
       "├─GELU: 1-2                              --\n",
       "├─BatchNorm2d: 1-3                       2\n",
       "=================================================================\n",
       "Total params: 515\n",
       "Trainable params: 515\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_conv = conv(512, 1, 1, 1, 0).cuda()\n",
    "print(vl_conv)\n",
    "summary(model=vl_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1bdb87d-5a21-44eb-9c5d-02960fe3da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tokens = vl_conv(out)\n",
    "\n",
    "img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd6549-72d0-4220-83a3-5315e74cde90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c233bbbf-f4c0-481a-89c2-6132b01eb737",
   "metadata": {},
   "source": [
    "#### Token Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "401abc12-3cb4-47ed-ad2b-d5a780331c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Transformer FeedForward block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim, \n",
    "        mlp_dim, \n",
    "        out_dim, \n",
    "        dropout_rate, \n",
    "        activation=nn.GELU()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, mlp_dim)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(mlp_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TokenLearnerModuleV11(nn.Module):\n",
    "    \"\"\"\n",
    "        Re-Implementation if TokenLearner version 1.1\n",
    "        - MLP (2 dense layers with gelu) for generating attention map\n",
    "        - uses softmax instead of sigmoid\n",
    "        - Should be ~ 34K parameters\n",
    "        \n",
    "        Adapted from https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py\n",
    "        \n",
    "        reference: @misc{ryoo2022tokenlearner,\n",
    "              title={TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?}, \n",
    "              author={Michael S. Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},\n",
    "              year={2022},\n",
    "              eprint={2106.11297},\n",
    "              archivePrefix={arXiv},\n",
    "              primaryClass={cs.CV}\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_shape,\n",
    "        num_tokens:int=8, \n",
    "        bottleneck_dim=64, \n",
    "        dropout_rate=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.feature_shape = feature_shape\n",
    "        \n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "        # self.token_masking = MlpBlock(\n",
    "        #     in_dim=self.feature_shape[-1],\n",
    "        #     mlp_dim=self.bottleneck_dim,\n",
    "        #     out_dim=self.num_tokens,\n",
    "        #     dropout_rate=self.dropout_rate\n",
    "        # )\n",
    "        \n",
    "        self.token_masking = FeedFowardLayer(\n",
    "            in_dim=self.feature_shape[-1],\n",
    "            mlp_dim=self.bottleneck_dim,\n",
    "            out_dim=self.num_tokens,            \n",
    "            activation_fn=\"GELU\"\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, deterministic:bool=True):\n",
    "        if inputs.dim() == 4:\n",
    "            n, h, w, c = inputs.size()\n",
    "            inputs = inputs.view(n, h * w, c)\n",
    "\n",
    "        selected = inputs\n",
    "\n",
    "        selected = self.layer_norm(selected)\n",
    "        selected = self.token_masking(selected)\n",
    "\n",
    "        selected = selected.view(self.feature_shape[0], -1, self.num_tokens)  # Shape: [bs, h*w, n_token].\n",
    "        selected = selected.transpose(1, 2)  # Shape: [bs, n_token, h*w].\n",
    "        selected = F.softmax(selected, dim=-1)\n",
    "\n",
    "        feat = inputs\n",
    "        feat = feat.view(self.feature_shape[0], -1, self.feature_shape[-1])  # Shape: [bs, h*w, c].\n",
    "\n",
    "        feat = torch.einsum('...si,...id->...sd', [selected, feat])\n",
    "\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "363d5aca-4d00-4bfa-808a-b9ce8c89ce16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 7, 7)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, C, H, W = out.shape\n",
    "\n",
    "N, C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb960649-ad52-4a6f-be9a-279263b0ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenLearnerModuleV11(\n",
      "  (layer_norm): LayerNormalization(\n",
      "    (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (token_masking): FeedFowardLayer(\n",
      "    (linear_1): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (activation): GELU(approximate='none')\n",
      "    (linear_2): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TokenLearnerModuleV11                    --\n",
       "├─LayerNormalization: 1-1                --\n",
       "│    └─LayerNorm: 2-1                    1,024\n",
       "├─FeedFowardLayer: 1-2                   --\n",
       "│    └─Linear: 2-2                       32,832\n",
       "│    └─GELU: 2-3                         --\n",
       "│    └─Linear: 2-4                       520\n",
       "│    └─Dropout: 2-5                      --\n",
       "=================================================================\n",
       "Total params: 34,376\n",
       "Trainable params: 34,376\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokL_v11 = TokenLearnerModuleV11(feature_shape=(N, H*W, C))\n",
    "print(tokL_v11)\n",
    "summary(model=tokL_v11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb43cec1-32e2-4f1d-bb5a-3ba026a57fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor with the shape [batch_size, height, width, channels]\n",
    "dummy_input = torch.randn(N, H * W, C)\n",
    "\n",
    "\n",
    "# Initialize the TokenLearnerModuleV11\n",
    "model = TokenLearnerModuleV11(feature_shape=(N, H*W, C))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Pass the dummy input through the model\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2696e7b-5907-461b-b6aa-15a1ca393fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4128e-0404-4823-b705-9b9777db2ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e045e3a-4d75-454a-9fdd-c5205f332a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ce81c2-ecfd-49cf-8613-b806d585dfb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### RT-1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4840f4a2-f578-4816-b623-c1d12cee41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_bacnbone:str=\"resnet18\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = TextEncoder()\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_encoder = FiLMEncoder()\n",
    "        \n",
    "        # Vision-Language tokens extractor\n",
    "        self.vl_conv = conv(self.image_encoder.n_channels, 1, 1, 1, 0)\n",
    "        \n",
    "        # Token Learner\n",
    "        self.token_learner = TokenLearner()\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.transformer = TransformerDecoder()\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids, imgs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        text_enc = self.text_encoder(\n",
    "            inp_ids=input_ids,\n",
    "            mask=attn_mask,\n",
    "            tok_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Generage image tokens\n",
    "        img_tokens = self.image_encoder(\n",
    "            x= imgs,\n",
    "            description= text_enc\n",
    "        )\n",
    "        \n",
    "        # Vision-Language tokens extractor\n",
    "        img_tokens = self.vl_conv(img_tokens)\n",
    "        \n",
    "        # Extract learned tokens\n",
    "        learned_tokens  = self.tok(img_tokens)\n",
    "        \n",
    "        return learned_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d56dd9db-abff-4cb9-b8af-b1b68e3e7ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Param #                   Trainable\n",
       "==============================================================================================================\n",
       "RT1Encoder                                                   --                        Partial\n",
       "├─TextEncoder: 1-1                                           --                        False\n",
       "│    └─BertModel: 2-1                                        --                        False\n",
       "│    │    └─BertEmbeddings: 3-1                              (7,945,728)               False\n",
       "│    │    └─BertEncoder: 3-2                                 (3,159,040)               False\n",
       "│    │    └─BertPooler: 3-3                                  (65,792)                  False\n",
       "│    └─Dropout: 2-2                                          --                        --\n",
       "├─FiLMEncoder: 1-2                                           --                        Partial\n",
       "│    └─Linear: 2-3                                           101,376                   True\n",
       "│    └─FeatureExtractor: 2-4                                 --                        Partial\n",
       "│    │    └─ResNet: 3-4                                      11,689,512                Partial\n",
       "│    │    └─Sequential: 3-5                                  (11,176,512)              False\n",
       "│    └─ModuleList: 2-5                                       --                        True\n",
       "│    │    └─ResBlock: 3-6                                    2,624,512                 True\n",
       "│    │    └─ResBlock: 3-7                                    2,624,512                 True\n",
       "│    │    └─ResBlock: 3-8                                    2,624,512                 True\n",
       "├─Sequential: 1-3                                            --                        True\n",
       "│    └─Conv2d: 2-6                                           513                       True\n",
       "│    └─GELU: 2-7                                             --                        --\n",
       "│    └─BatchNorm2d: 2-8                                      2                         True\n",
       "├─TokenLearner: 1-4                                          --                        True\n",
       "│    └─Sequential: 2-9                                       --                        True\n",
       "│    │    └─Conv2d: 3-9                                      4,284,896                 True\n",
       "│    │    └─GELU: 3-10                                       --                        --\n",
       "│    │    └─Conv2d: 3-11                                     8,280                     True\n",
       "├─TransformerDecoder: 1-5                                    --                        True\n",
       "│    └─ModuleList: 2-10                                      --                        True\n",
       "│    │    └─TransformerDecoderLayer: 3-12                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-13                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-14                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-15                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-16                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-17                    3,154,432                 True\n",
       "│    └─LayerNormalization: 2-11                              --                        True\n",
       "│    │    └─LayerNorm: 3-18                                  1,024                     True\n",
       "==============================================================================================================\n",
       "Total params: 65,232,803\n",
       "Trainable params: 31,709,219\n",
       "Non-trainable params: 33,523,584\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = RT1Encoder()\n",
    "summary(model=encoder, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405de1c-ea97-4908-aa25-7858478627c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a8bea-a4ac-45a1-affd-5047ff76e85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029e620f-5541-4a49-acdf-43ec4b3cce02",
   "metadata": {},
   "source": [
    "### RT-1 Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2c336-d441-4f0d-a879-e4337d2abc7e",
   "metadata": {},
   "source": [
    "#### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ebf21e-d456-4f85-bb0c-fd1056c517f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TransformerDecoder                       --\n",
       "├─ModuleList: 1-1                        --\n",
       "│    └─TransformerDecoderLayer: 2-1      --\n",
       "│    │    └─LayerNormalization: 3-1      1,024\n",
       "│    │    └─MultiheadAttention: 3-2      1,050,624\n",
       "│    │    └─Dropout: 3-3                 --\n",
       "│    │    └─LayerNormalization: 3-4      1,024\n",
       "│    │    └─MultiheadAttention: 3-5      1,050,624\n",
       "│    │    └─Dropout: 3-6                 --\n",
       "│    │    └─LayerNormalization: 3-7      1,024\n",
       "│    │    └─FeedFowardLayer: 3-8         1,050,112\n",
       "│    │    └─Dropout: 3-9                 --\n",
       "│    └─TransformerDecoderLayer: 2-2      --\n",
       "│    │    └─LayerNormalization: 3-10     1,024\n",
       "│    │    └─MultiheadAttention: 3-11     1,050,624\n",
       "│    │    └─Dropout: 3-12                --\n",
       "│    │    └─LayerNormalization: 3-13     1,024\n",
       "│    │    └─MultiheadAttention: 3-14     1,050,624\n",
       "│    │    └─Dropout: 3-15                --\n",
       "│    │    └─LayerNormalization: 3-16     1,024\n",
       "│    │    └─FeedFowardLayer: 3-17        1,050,112\n",
       "│    │    └─Dropout: 3-18                --\n",
       "│    └─TransformerDecoderLayer: 2-3      --\n",
       "│    │    └─LayerNormalization: 3-19     1,024\n",
       "│    │    └─MultiheadAttention: 3-20     1,050,624\n",
       "│    │    └─Dropout: 3-21                --\n",
       "│    │    └─LayerNormalization: 3-22     1,024\n",
       "│    │    └─MultiheadAttention: 3-23     1,050,624\n",
       "│    │    └─Dropout: 3-24                --\n",
       "│    │    └─LayerNormalization: 3-25     1,024\n",
       "│    │    └─FeedFowardLayer: 3-26        1,050,112\n",
       "│    │    └─Dropout: 3-27                --\n",
       "│    └─TransformerDecoderLayer: 2-4      --\n",
       "│    │    └─LayerNormalization: 3-28     1,024\n",
       "│    │    └─MultiheadAttention: 3-29     1,050,624\n",
       "│    │    └─Dropout: 3-30                --\n",
       "│    │    └─LayerNormalization: 3-31     1,024\n",
       "│    │    └─MultiheadAttention: 3-32     1,050,624\n",
       "│    │    └─Dropout: 3-33                --\n",
       "│    │    └─LayerNormalization: 3-34     1,024\n",
       "│    │    └─FeedFowardLayer: 3-35        1,050,112\n",
       "│    │    └─Dropout: 3-36                --\n",
       "│    └─TransformerDecoderLayer: 2-5      --\n",
       "│    │    └─LayerNormalization: 3-37     1,024\n",
       "│    │    └─MultiheadAttention: 3-38     1,050,624\n",
       "│    │    └─Dropout: 3-39                --\n",
       "│    │    └─LayerNormalization: 3-40     1,024\n",
       "│    │    └─MultiheadAttention: 3-41     1,050,624\n",
       "│    │    └─Dropout: 3-42                --\n",
       "│    │    └─LayerNormalization: 3-43     1,024\n",
       "│    │    └─FeedFowardLayer: 3-44        1,050,112\n",
       "│    │    └─Dropout: 3-45                --\n",
       "│    └─TransformerDecoderLayer: 2-6      --\n",
       "│    │    └─LayerNormalization: 3-46     1,024\n",
       "│    │    └─MultiheadAttention: 3-47     1,050,624\n",
       "│    │    └─Dropout: 3-48                --\n",
       "│    │    └─LayerNormalization: 3-49     1,024\n",
       "│    │    └─MultiheadAttention: 3-50     1,050,624\n",
       "│    │    └─Dropout: 3-51                --\n",
       "│    │    └─LayerNormalization: 3-52     1,024\n",
       "│    │    └─FeedFowardLayer: 3-53        1,050,112\n",
       "│    │    └─Dropout: 3-54                --\n",
       "├─LayerNormalization: 1-2                --\n",
       "│    └─LayerNorm: 2-7                    1,024\n",
       "=================================================================\n",
       "Total params: 18,927,616\n",
       "Trainable params: 18,927,616\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = TransformerDecoder()\n",
    "summary(model=dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab8990-184b-4854-980e-ef4887300b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f252ad6-5c3a-44ce-ab3d-4d4d75d1960a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e365900-13ff-4dd8-b789-6f45b15c0564",
   "metadata": {},
   "source": [
    "#### Action Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0bc4-3be4-4b84-a52a-7235e9af947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14293b-b031-477b-a643-3811927b1bb1",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ec749-1fbf-4072-b191-b6e3ad2f243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.transformer = TransformerDecoder()\n",
    "        # \n",
    "        self.action_generator = ActionGenerator()\n",
    "        \n",
    "\n",
    "    def _positional_encoding(\n",
    "        self,\n",
    "        seq, \n",
    "        dim, \n",
    "        temperature = 10000, \n",
    "        device = None, \n",
    "        dtype = torch.float32\n",
    "    ):\n",
    "        n = torch.arange(seq, device = device)\n",
    "        omega = torch.arange(dim // 2, device = device) / (dim // 2 - 1)\n",
    "        omega = 1. / (temperature ** omega)\n",
    "\n",
    "        n = n[:, None] * omega[None, :]\n",
    "        pos_emb = torch.cat((n.sin(), n.cos()), dim = 1)\n",
    "        \n",
    "        return pos_emb.type(dtype)\n",
    "\n",
    "    \n",
    "    def forward(self, instructions, imgs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f893fa-9324-4244-aad2-a66db2243700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cde5f94-8a3c-458d-94da-9655d7ddb2d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RT-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033aac5-7312-47d8-8ff8-8044ebb2fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = RT1Encder()\n",
    "        self.decoder = RT1Decoder()\n",
    "        \n",
    "    def forward(self, imgs, instruction):\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd461-7028-49de-bcb9-e8464433f548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a346bbe-d174-40a9-a807-e2cd63665ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bebcb-a94a-4272-b68d-5eeee9dbd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMF-BE",
   "language": "python",
   "name": "smf-be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
