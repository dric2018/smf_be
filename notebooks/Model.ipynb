{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11449575-6314-4708-9193-b279891e836c",
   "metadata": {},
   "source": [
    "## Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f302c0d-40f1-4bac-bf57-df54c89e72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 20 03:18:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    54W / 300W |    725MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     34827      C   ...nvs/smf_be/bin/python3.10      722MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ee382-8c34-4130-99d3-80d5abdfbf1a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2664559-8789-4785-83d6-26327fe5ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmppwrkadki\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmppwrkadki/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from einops import pack, unpack, repeat, reduce, rearrange\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModel, AdamW, AutoConfig, get_linear_schedule_with_warmup)\n",
    "\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09a35d4-17a4-437a-ad75-cfdbf9215827",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef863cc-4128-49a3-a156-fbdc859a9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataloader import BEDataset, BEDataModule\n",
    "from transformer import PositionalEncoder, TransformerDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b26cc-b5a8-4a5a-9e6c-6d2721d34c58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e68541d-e1c2-48df-a18d-c37c9da42002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ID</th>\n",
       "      <th>in_state</th>\n",
       "      <th>goal_state</th>\n",
       "      <th>action_description</th>\n",
       "      <th>motor_cmd</th>\n",
       "      <th>len_action_desc</th>\n",
       "      <th>len_motor_cmd</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the fork to the right of buttermilk</td>\n",
       "      <td>:FORK GREEN POSE-9 :BUTTERMILK GREEN POSE-2 :F...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>move the bottle backwards</td>\n",
       "      <td>:BOTTLE RED POSE-2 :BOTTLE  #'*backward-transf...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4235</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the bottle to the left of breakfast-cereal</td>\n",
       "      <td>:BOTTLE RED POSE-7 :BREAKFAST-CEREAL BLUE POSE...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6990</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the milk in front of bottle</td>\n",
       "      <td>:MILK BLUE POSE-8 :BOTTLE RED POSE-4 :MILK  #'...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7096</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>put the cup in front of glasses</td>\n",
       "      <td>:CUP GREEN POSE-6 :GLASSES RED POSE-2 :CUP  #'...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_ID  in_state  goal_state  \\\n",
       "0       7294         0          10   \n",
       "1        405         0           8   \n",
       "2       4235         0          10   \n",
       "3       6990         0          10   \n",
       "4       7096         0          10   \n",
       "\n",
       "                               action_description  \\\n",
       "0         put the fork to the right of buttermilk   \n",
       "1                       move the bottle backwards   \n",
       "2  put the bottle to the left of breakfast-cereal   \n",
       "3                 put the milk in front of bottle   \n",
       "4                 put the cup in front of glasses   \n",
       "\n",
       "                                           motor_cmd  len_action_desc  \\\n",
       "0  :FORK GREEN POSE-9 :BUTTERMILK GREEN POSE-2 :F...                8   \n",
       "1  :BOTTLE RED POSE-2 :BOTTLE  #'*backward-transf...                4   \n",
       "2  :BOTTLE RED POSE-7 :BREAKFAST-CEREAL BLUE POSE...                8   \n",
       "3  :MILK BLUE POSE-8 :BOTTLE RED POSE-4 :MILK  #'...                7   \n",
       "4  :CUP GREEN POSE-6 :GLASSES RED POSE-2 :CUP  #'...                7   \n",
       "\n",
       "   len_motor_cmd version  \n",
       "0             11      v2  \n",
       "1              8      v1  \n",
       "2             11      v2  \n",
       "3             11      v2  \n",
       "4             11      v2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv(os.path.join(config.DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7cd7ee-95e6-450a-b7bf-35a626daf357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4876"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building data object\n",
    "ds = BEDataset(\n",
    "    df=csv    \n",
    ")\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ccf421-7092-4d19-b7a8-84d4864f34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  4876\n",
      "====================================================================================================\n",
      "ID\t:  2636\n",
      ">> InState\t:  torch.Size([3, 224, 224])\n",
      ">> Desc\t:\n",
      "{'ids': tensor([ 101, 2404, 1996, 4605, 2006, 2327, 1997, 5127,  102,    0,    0,    0,\n",
      "           0,    0,    0]),\n",
      " 'length': 7,\n",
      " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]),\n",
      " 'raw': 'put the bowl on top of plate',\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      ">> Cmd\t:\n",
      "{'ids': tensor([ 0, 21, 31, 47, 24, 31, 27, 21, 42, 24,  2,  2,  2,  2,  2]),\n",
      " 'length': 11,\n",
      " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]),\n",
      " 'raw': \":BOWL RED POSE-6 :PLATE RED POSE-4 :BOWL  #'*on-transformation*  \"\n",
      "        ':PLATE'}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# fetching example\n",
    "rand_idx = np.random.randint(low=0, high=len(ds))\n",
    "ex = ds[rand_idx]\n",
    "\n",
    "print(\"Dataset size: \", len(ds))\n",
    "print(\"=\"*100)\n",
    "print(\"ID\\t: \", ex[\"sample_id\"])\n",
    "print(\">> InState\\t: \", ex[\"in_state\"].shape)\n",
    "print(\">> Desc\\t:\")\n",
    "pprint(ex[\"action_desc\"])\n",
    "print(\">> Cmd\\t:\")\n",
    "pprint(ex[\"motor_cmd\"])\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425b526-43d7-483b-b3bc-02028bfb4b7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820bb68f-3341-4d6e-9cea-80dbdf2de002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # examples: 4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training on 3610 samples.\n",
      "INFO:root:Validating on 1266 samples.\n"
     ]
    }
   ],
   "source": [
    "dm = BEDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "383eaa10-47d7-4620-928e-8fbcd6889d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      ">> train data loader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "# train batches\t: 451\n",
      "In \t\t\t:  torch.Size([8, 3, 224, 224])\n",
      "Action desc \t\t:  torch.Size([8, 16])\n",
      "Action desc (len) \t:  torch.Size([8])\n",
      "CMD \t\t\t:  torch.Size([8, 16])\n",
      "CMD(len) \t\t:  torch.Size([8])\n",
      "\n",
      "IDs & decided tokens\n",
      "[101, 2404, 1996, 5835, 2000, 1996, 2187, 1997, 14690, 7068, 102, 0, 0, 0, 0, 0]\n",
      "put the bottle to the left of spatula\n",
      "\n",
      "[0, 8, 46, 50, 10, 31, 40, 8, 43, 10, 2, 2, 2, 2, 2, 2]\n",
      ":BOTTLE BLUE POSE-7 :SPATULA RED POSE-2 :BOTTLE #'*leftward-transformation* :SPATULA\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "logging.info(\"\\n>> train data loader\")\n",
    "print(f\"# train batches\\t: {len(dm.train_dataloader())}\")\n",
    "for data in dm.train_dataloader():\n",
    "    # pprint(data)\n",
    "    sample_id, in_state, ad, cmd = data[\"sample_id\"], data[\"in_state\"], data[\"action_desc\"], data[\"motor_cmd\"]\n",
    "    print(\"In \\t\\t\\t: \", in_state.shape)\n",
    "    print(\"Action desc \\t\\t: \", ad[\"ids\"].shape)\n",
    "    print(\"Action desc (len) \\t: \", ad[\"length\"].shape)\n",
    "    print(\"CMD \\t\\t\\t: \", cmd[\"ids\"].shape)\n",
    "    print(\"CMD(len) \\t\\t: \", cmd[\"length\"].shape)\n",
    "    break\n",
    "\n",
    "print(\"\\nIDs & decided tokens\")\n",
    "for data in dm.train_dataloader():\n",
    "    print(data[\"action_desc\"][\"ids\"][0].tolist())\n",
    "    print(dm.train_ds._decode_inputs(data[\"action_desc\"][\"ids\"][0].tolist()))\n",
    "    print()\n",
    "    print(data[\"motor_cmd\"][\"ids\"][0].tolist())\n",
    "    print(dm.train_ds._decode_outputs(data[\"motor_cmd\"][\"ids\"][0].tolist()))\n",
    "\n",
    "    break\n",
    "    \n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b32ff7-02ee-4fa3-b3b9-7885f558072b",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcf1ad-8562-458b-b07c-226d1ed3dbe0",
   "metadata": {},
   "source": [
    "<!-- ![RT1 model architecture](../../imgs/rt1+.png) -->\n",
    "<center>\n",
    "    <img src=\"../../imgs/rt1+.png\" alt=\"RT1 model architecture\" width=\"500\" height=\"300\">\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cea30-c012-4a53-a94b-7651270e4c38",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd8cfa-4153-48e7-bbad-c11ba0c6357d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be96cc06-3693-4e52-8000-23a28efc0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(model:nn.Module):\n",
    "    return nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "def conv(ic, oc, k, s, p, activation:str=\"GELU\"):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ic, oc, k, s, p),\n",
    "        getattr(nn, activation)(),\n",
    "        nn.BatchNorm2d(oc),\n",
    "    )\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained:bool=True, \n",
    "        arch:str=\"resnet34\",\n",
    "        freeze:bool=True\n",
    "    ):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.freeze = freeze\n",
    "\n",
    "        if self.pretrained:\n",
    "            self.arch   = getattr(models, arch)(weights=\"IMAGENET1K_V1\")\n",
    "            self.fe     = get_backbone(model=self.arch)\n",
    "        else:\n",
    "            self.fe = nn.Sequential(\n",
    "            conv(3, 128, 5, 2, 2),\n",
    "            conv(128, 128, 3, 2, 1),\n",
    "            conv(128, 128, 3, 2, 1),\n",
    "            conv(128, 128, 3, 1, 1),\n",
    "            conv(128, 128, 3, 1, 1),\n",
    "        )\n",
    "            \n",
    "        if self.freeze:\n",
    "            self._freeze_model()\n",
    "            \n",
    "    def _freeze_model(self):\n",
    "        for param in self.fe.parameters():\n",
    "            param.requires_grad = False \n",
    "\n",
    "    def forward(self, x, flat_out:bool=False):\n",
    "        if self.pretrained:\n",
    "            enc = self.fe(x)\n",
    "            if flat_out:\n",
    "                return torch.flatten(enc, 1)\n",
    "            else:\n",
    "                return enc\n",
    "        else:\n",
    "            return self.fe(x)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_channels, n_classes):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(prev_channels, 512, 1, 1, 0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, n_classes))\n",
    "\n",
    "    def forward(self, x, return_feats:bool=True):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        feats = self.global_max_pool(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return torch.flatten(feats, 1)\n",
    "        else:\n",
    "            x = feats.view(feats.size(0), feats.size(1))\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d55d73ba-efef-4d32-81cb-520974830c74",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Param #                   Trainable\n",
       "==============================================================================================================\n",
       "FeatureExtractor                                             --                        Partial\n",
       "├─EfficientNet: 1-1                                          --                        Partial\n",
       "│    └─Sequential: 2-1                                       --                        False\n",
       "│    │    └─Conv2dNormActivation: 3-1                        (1,160)                   False\n",
       "│    │    └─Sequential: 3-2                                  (3,504)                   False\n",
       "│    │    └─Sequential: 3-3                                  (48,118)                  False\n",
       "│    │    └─Sequential: 3-4                                  (110,912)                 False\n",
       "│    │    └─Sequential: 3-5                                  (638,700)                 False\n",
       "│    │    └─Sequential: 3-6                                  (1,387,760)               False\n",
       "│    │    └─Sequential: 3-7                                  (4,628,964)               False\n",
       "│    │    └─Sequential: 3-8                                  (3,284,218)               False\n",
       "│    │    └─Conv2dNormActivation: 3-9                        (592,896)                 False\n",
       "│    └─AdaptiveAvgPool2d: 2-2                                --                        --\n",
       "│    └─Sequential: 2-3                                       --                        True\n",
       "│    │    └─Dropout: 3-10                                    --                        --\n",
       "│    │    └─Linear: 3-11                                     1,537,000                 True\n",
       "├─Sequential: 1-2                                            10,696,232                False\n",
       "│    └─Sequential: 2-4                                       (recursive)               False\n",
       "│    │    └─Conv2dNormActivation: 3-12                       (recursive)               False\n",
       "│    │    └─Sequential: 3-13                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-14                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-15                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-16                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-17                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-18                                 (recursive)               False\n",
       "│    │    └─Sequential: 3-19                                 (recursive)               False\n",
       "│    │    └─Conv2dNormActivation: 3-20                       (recursive)               False\n",
       "==============================================================================================================\n",
       "Total params: 22,929,464\n",
       "Trainable params: 1,537,000\n",
       "Non-trainable params: 21,392,464\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = FeatureExtractor(pretrained=True, arch=\"efficientnet_b3\").cuda()\n",
    "\n",
    "summary(fe, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34e9d6e5-ca31-4b20-8fe1-07412fcb4f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536, 7, 7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ftrs = fe(ex[\"in_state\"].unsqueeze(0).cuda())\n",
    "\n",
    "img_ftrs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88aa1a-8e3a-4abf-998d-edda663b0a55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Film Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13693bd3-cfb6-4f7d-be30-5fe251079c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FiLMBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        beta = beta.view(x.size(0), x.size(1), 1, 1)\n",
    "        gamma = gamma.view(x.size(0), x.size(1), 1, 1)\n",
    "\n",
    "        x = gamma * x + beta\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c64a2-4b72-4cfe-a973-9b9f22932605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Film Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1990061-09b1-42fe-84aa-ed92c4b880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Courtesy of [Kim Minjong](https://github.com/caffeinism):\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_place, out_place):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_place, out_place, 1, 1, 0)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_place, out_place, 3, 1, 1)\n",
    "        self.norm2 = nn.BatchNorm2d(out_place)\n",
    "        self.film = FiLMBlock()\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, beta, gamma):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.film(x, beta, gamma)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d547d-b935-42b3-bafa-ded2cddaec51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6505cc1-9649-490b-922c-d31deda161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, dropout_rate:float=config.TEXT_ENC_DROPOUT, freeze:bool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze = freeze\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(config.LANG_MODEL_NAME)\n",
    "        self.text_encoder = AutoModel.from_pretrained(config.LANG_MODEL_NAME, config=model_config)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        if self.freeze:\n",
    "            self._freeze_model()\n",
    "\n",
    "    def _freeze_model(self):\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False \n",
    "        \n",
    "    def forward(self, inp_ids, mask, tok_type_ids):\n",
    "        # embed NL instructions\n",
    "        text_enc = self.text_encoder(\n",
    "            input_ids=inp_ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=tok_type_ids\n",
    "        ).pooler_output\n",
    "        \n",
    "        # print(text_enc.shape)\n",
    "        text_enc = self.dropout(text_enc)\n",
    "        \n",
    "        return text_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cbe10a4-0a68-429f-a18c-dfb4012920ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Param #                   Trainable\n",
       "=========================================================================================================\n",
       "TextEncoder                                             --                        False\n",
       "├─BertModel: 1-1                                        --                        False\n",
       "│    └─BertEmbeddings: 2-1                              --                        False\n",
       "│    │    └─Embedding: 3-1                              (7,813,632)               False\n",
       "│    │    └─Embedding: 3-2                              (131,072)                 False\n",
       "│    │    └─Embedding: 3-3                              (512)                     False\n",
       "│    │    └─LayerNorm: 3-4                              (512)                     False\n",
       "│    │    └─Dropout: 3-5                                --                        --\n",
       "│    └─BertEncoder: 2-2                                 --                        False\n",
       "│    │    └─ModuleList: 3-6                             (3,159,040)               False\n",
       "│    └─BertPooler: 2-3                                  --                        False\n",
       "│    │    └─Linear: 3-7                                 (65,792)                  False\n",
       "│    │    └─Tanh: 3-8                                   --                        --\n",
       "├─Dropout: 1-2                                          --                        --\n",
       "=========================================================================================================\n",
       "Total params: 11,170,560\n",
       "Trainable params: 0\n",
       "Non-trainable params: 11,170,560\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextEncoder(freeze=True).cuda()\n",
    "te._freeze_model()\n",
    "summary(model=te, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c58d15f0-b9f5-4a37-8dc4-2eb768d42ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': 'put the bowl on top of plate',\n",
       " 'ids': tensor([ 101, 2404, 1996, 4605, 2006, 2327, 1997, 5127,  102,    0,    0,    0,\n",
       "            0,    0,    0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'length': 7}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[\"action_desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1790e904-591d-4ee3-8ae3-f9ee204dbe31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb = te(\n",
    "    inp_ids=ex[\"action_desc\"][\"ids\"].unsqueeze(0).cuda(),\n",
    "    mask=ex[\"action_desc\"][\"mask\"].unsqueeze(0).cuda(),\n",
    "    tok_type_ids=ex[\"action_desc\"][\"token_type_ids\"].unsqueeze(0).cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50303d89-95ce-430a-8e74-5df3de3d8349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9761f-f182-4918-81c1-24aac2cc73e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Film Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cfec091-c164-480e-b96a-76af494e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Adapted from: https://github.com/caffeinism/FiLM-pytorch/blob/master/networks.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_res_blocks:int=3,\n",
    "        out_dim:int=256,\n",
    "        n_channels:int=512,\n",
    "        dim_description:int=32,\n",
    "        arch:str=\"resnet18\"\n",
    "    ):\n",
    "        super(FiLMEncoder, self).__init__()\n",
    "        \n",
    "        if arch in [\"resnet18\", \"resnet34\"]:\n",
    "            n_channels = 512\n",
    "        elif \"resnet\" in arch:\n",
    "            n_channels = 2048\n",
    "        elif \"convnext\" in arch:\n",
    "            n_channels = 768\n",
    "\n",
    "        self.dim_description = dim_description\n",
    "        self.film_generator = nn.Linear(self.dim_description, 2 * n_res_blocks * n_channels)\n",
    "        self.feature_extractor = FeatureExtractor(arch=arch)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_res_blocks):\n",
    "            self.res_blocks.append(ResBlock(n_channels + 2, n_channels))\n",
    "\n",
    "        # self.head = Head(n_channels, out_dim)\n",
    "\n",
    "        self.n_res_blocks = n_res_blocks\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "    def forward(self, x, description):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.feature_extractor(x)\n",
    "        film_vector = self.film_generator(description).view(\n",
    "            batch_size, self.n_res_blocks, 2, self.n_channels)\n",
    "        \n",
    "        d = x.size(-1)\n",
    "        coordinate = torch.arange(-1, 1 + 0.00001, 2 / (d-1)).cuda()\n",
    "        coordinate_x = coordinate.expand(batch_size, 1, d, d)\n",
    "        coordinate_y = coordinate.view(d, 1).expand(batch_size, 1, d, d)\n",
    "        # print(f\"x.shape: {x.shape} - coordinate_x.shape: {coordinate_x.shape} - coordinate_y: {coordinate_y.shape}\")\n",
    "\n",
    "        for i, res_block in enumerate(self.res_blocks):\n",
    "            beta = film_vector[:, i, 0, :]\n",
    "            gamma = film_vector[:, i, 1, :]\n",
    "\n",
    "            x = torch.cat([x, coordinate_x, coordinate_y], 1)\n",
    "            x = res_block(x, beta, gamma)\n",
    "        \n",
    "        print(\"pre-classifier: \", x.shape)\n",
    "        feats = x #self.head(x, return_feats=False)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09a904d1-b1ef-40b2-9344-f2580105ee41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "FiLMEncoder                                        --\n",
       "├─Linear: 1-1                                      526,336\n",
       "├─FeatureExtractor: 1-2                            --\n",
       "│    └─ResNet: 2-1                                 --\n",
       "│    │    └─Conv2d: 3-1                            9,408\n",
       "│    │    └─BatchNorm2d: 3-2                       128\n",
       "│    │    └─ReLU: 3-3                              --\n",
       "│    │    └─MaxPool2d: 3-4                         --\n",
       "│    │    └─Sequential: 3-5                        147,968\n",
       "│    │    └─Sequential: 3-6                        525,568\n",
       "│    │    └─Sequential: 3-7                        2,099,712\n",
       "│    │    └─Sequential: 3-8                        8,393,728\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 --\n",
       "│    │    └─Linear: 3-10                           513,000\n",
       "│    └─Sequential: 2-2                             11,176,512\n",
       "│    │    └─Conv2d: 3-11                           (recursive)\n",
       "│    │    └─BatchNorm2d: 3-12                      (recursive)\n",
       "│    │    └─ReLU: 3-13                             --\n",
       "│    │    └─MaxPool2d: 3-14                        --\n",
       "│    │    └─Sequential: 3-15                       (recursive)\n",
       "│    │    └─Sequential: 3-16                       (recursive)\n",
       "│    │    └─Sequential: 3-17                       (recursive)\n",
       "│    │    └─Sequential: 3-18                       (recursive)\n",
       "├─ModuleList: 1-3                                  --\n",
       "│    └─ResBlock: 2-3                               --\n",
       "│    │    └─Conv2d: 3-19                           263,680\n",
       "│    │    └─ReLU: 3-20                             --\n",
       "│    │    └─Conv2d: 3-21                           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-22                      1,024\n",
       "│    │    └─FiLMBlock: 3-23                        --\n",
       "│    │    └─ReLU: 3-24                             --\n",
       "│    └─ResBlock: 2-4                               --\n",
       "│    │    └─Conv2d: 3-25                           263,680\n",
       "│    │    └─ReLU: 3-26                             --\n",
       "│    │    └─Conv2d: 3-27                           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-28                      1,024\n",
       "│    │    └─FiLMBlock: 3-29                        --\n",
       "│    │    └─ReLU: 3-30                             --\n",
       "===========================================================================\n",
       "Total params: 28,641,384\n",
       "Trainable params: 28,641,384\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_encoder = FiLMEncoder(\n",
    "    arch=\"resnet18\",\n",
    "    n_res_blocks=2,\n",
    "    dim_description=256 # (emb) action_description_size\n",
    ").cuda()\n",
    "\n",
    "# print(film_encoder)\n",
    "summary(model=film_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b70f6a7-c6b5-4743-8ecf-9cfa122dd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-classifier:  torch.Size([1, 512, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = film_encoder(\n",
    "    x= ex[\"in_state\"].unsqueeze(0).cuda(),\n",
    "    description= emb\n",
    ")\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6aaceb0-0f2b-487e-9127-c60c7cfe4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Conv2d: 1-1                            513\n",
       "├─GELU: 1-2                              --\n",
       "├─BatchNorm2d: 1-3                       2\n",
       "=================================================================\n",
       "Total params: 515\n",
       "Trainable params: 515\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_conv = conv(512, 1, 1, 1, 0).cuda()\n",
    "print(vl_conv)\n",
    "summary(model=vl_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1bdb87d-5a21-44eb-9c5d-02960fe3da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 7])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tokens = vl_conv(out)\n",
    "\n",
    "img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd6549-72d0-4220-83a3-5315e74cde90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c233bbbf-f4c0-481a-89c2-6132b01eb737",
   "metadata": {},
   "source": [
    "#### Token Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "401abc12-3cb4-47ed-ad2b-d5a780331c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLearner(nn.Module):\n",
    "    \"\"\"\n",
    "        TokenLearner version 1.1\n",
    "        MLP (2 dense layers with gelu) for generating attention map\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim:int=512,\n",
    "        ff_mult = 2,\n",
    "        num_output_tokens = 8,\n",
    "        num_layers = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim * ff_mult * num_output_tokens\n",
    "\n",
    "        self.num_output_tokens = num_output_tokens\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim * num_output_tokens, inner_dim, 1, groups = num_output_tokens),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(inner_dim, num_output_tokens, 1, groups = num_output_tokens),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, ps = pack_one(x, '* c h w')\n",
    "        x = repeat(x, 'b c h w -> b (g c) h w', g = self.num_output_tokens)\n",
    "        attn = self.net(x)\n",
    "\n",
    "        attn = rearrange(attn, 'b g h w -> b 1 g h w')\n",
    "        x = rearrange(x, 'b (g c) h w -> b c g h w', g = self.num_output_tokens)\n",
    "\n",
    "        x = reduce(x * attn, 'b c g h w -> b c g', 'mean')\n",
    "        x = unpack_one(x, ps, '* c n')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e5f39ff-c7aa-460c-a7c2-0e887b52f825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TokenLearner                             --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       4,284,896\n",
       "│    └─GELU: 2-2                         --\n",
       "│    └─Conv2d: 2-3                       8,280\n",
       "=================================================================\n",
       "Total params: 4,293,176\n",
       "Trainable params: 4,293,176\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokL = TokenLearner()\n",
    "summary(model=tokL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43cec1-32e2-4f1d-bb5a-3ba026a57fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2696e7b-5907-461b-b6aa-15a1ca393fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4128e-0404-4823-b705-9b9777db2ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e045e3a-4d75-454a-9fdd-c5205f332a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ce81c2-ecfd-49cf-8613-b806d585dfb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### RT-1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4840f4a2-f578-4816-b623-c1d12cee41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_bacnbone:str=\"resnet18\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = TextEncoder()\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_encoder = FiLMEncoder()\n",
    "        \n",
    "        # Vision-Language tokens extractor\n",
    "        self.vl_conv = conv(self.image_encoder.n_channels, 1, 1, 1, 0)\n",
    "        \n",
    "        # Token Learner\n",
    "        self.token_learner = TokenLearner()\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.transformer = TransformerDecoder()\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids, imgs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        text_enc = self.text_encoder(\n",
    "            inp_ids=input_ids,\n",
    "            mask=attn_mask,\n",
    "            tok_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Generage image tokens\n",
    "        img_tokens = self.image_encoder(\n",
    "            x= imgs,\n",
    "            description= text_enc\n",
    "        )\n",
    "        \n",
    "        # Vision-Language tokens extractor\n",
    "        img_tokens = self.vl_conv(img_tokens)\n",
    "        \n",
    "        # Extract learned tokens\n",
    "        learned_tokens  = self.tok(img_tokens)\n",
    "        \n",
    "        return learned_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d56dd9db-abff-4cb9-b8af-b1b68e3e7ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Param #                   Trainable\n",
       "==============================================================================================================\n",
       "RT1Encoder                                                   --                        Partial\n",
       "├─TextEncoder: 1-1                                           --                        False\n",
       "│    └─BertModel: 2-1                                        --                        False\n",
       "│    │    └─BertEmbeddings: 3-1                              (7,945,728)               False\n",
       "│    │    └─BertEncoder: 3-2                                 (3,159,040)               False\n",
       "│    │    └─BertPooler: 3-3                                  (65,792)                  False\n",
       "│    └─Dropout: 2-2                                          --                        --\n",
       "├─FiLMEncoder: 1-2                                           --                        Partial\n",
       "│    └─Linear: 2-3                                           101,376                   True\n",
       "│    └─FeatureExtractor: 2-4                                 --                        Partial\n",
       "│    │    └─ResNet: 3-4                                      11,689,512                Partial\n",
       "│    │    └─Sequential: 3-5                                  (11,176,512)              False\n",
       "│    └─ModuleList: 2-5                                       --                        True\n",
       "│    │    └─ResBlock: 3-6                                    2,624,512                 True\n",
       "│    │    └─ResBlock: 3-7                                    2,624,512                 True\n",
       "│    │    └─ResBlock: 3-8                                    2,624,512                 True\n",
       "├─Sequential: 1-3                                            --                        True\n",
       "│    └─Conv2d: 2-6                                           513                       True\n",
       "│    └─GELU: 2-7                                             --                        --\n",
       "│    └─BatchNorm2d: 2-8                                      2                         True\n",
       "├─TokenLearner: 1-4                                          --                        True\n",
       "│    └─Sequential: 2-9                                       --                        True\n",
       "│    │    └─Conv2d: 3-9                                      4,284,896                 True\n",
       "│    │    └─GELU: 3-10                                       --                        --\n",
       "│    │    └─Conv2d: 3-11                                     8,280                     True\n",
       "├─TransformerDecoder: 1-5                                    --                        True\n",
       "│    └─ModuleList: 2-10                                      --                        True\n",
       "│    │    └─TransformerDecoderLayer: 3-12                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-13                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-14                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-15                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-16                    3,154,432                 True\n",
       "│    │    └─TransformerDecoderLayer: 3-17                    3,154,432                 True\n",
       "│    └─LayerNormalization: 2-11                              --                        True\n",
       "│    │    └─LayerNorm: 3-18                                  1,024                     True\n",
       "==============================================================================================================\n",
       "Total params: 65,232,803\n",
       "Trainable params: 31,709,219\n",
       "Non-trainable params: 33,523,584\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = RT1Encoder()\n",
    "summary(model=encoder, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405de1c-ea97-4908-aa25-7858478627c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a8bea-a4ac-45a1-affd-5047ff76e85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029e620f-5541-4a49-acdf-43ec4b3cce02",
   "metadata": {},
   "source": [
    "### RT-1 Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2c336-d441-4f0d-a879-e4337d2abc7e",
   "metadata": {},
   "source": [
    "#### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ebf21e-d456-4f85-bb0c-fd1056c517f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TransformerDecoder                       --\n",
       "├─ModuleList: 1-1                        --\n",
       "│    └─TransformerDecoderLayer: 2-1      --\n",
       "│    │    └─LayerNormalization: 3-1      1,024\n",
       "│    │    └─MultiheadAttention: 3-2      1,050,624\n",
       "│    │    └─Dropout: 3-3                 --\n",
       "│    │    └─LayerNormalization: 3-4      1,024\n",
       "│    │    └─MultiheadAttention: 3-5      1,050,624\n",
       "│    │    └─Dropout: 3-6                 --\n",
       "│    │    └─LayerNormalization: 3-7      1,024\n",
       "│    │    └─FeedFowardLayer: 3-8         1,050,112\n",
       "│    │    └─Dropout: 3-9                 --\n",
       "│    └─TransformerDecoderLayer: 2-2      --\n",
       "│    │    └─LayerNormalization: 3-10     1,024\n",
       "│    │    └─MultiheadAttention: 3-11     1,050,624\n",
       "│    │    └─Dropout: 3-12                --\n",
       "│    │    └─LayerNormalization: 3-13     1,024\n",
       "│    │    └─MultiheadAttention: 3-14     1,050,624\n",
       "│    │    └─Dropout: 3-15                --\n",
       "│    │    └─LayerNormalization: 3-16     1,024\n",
       "│    │    └─FeedFowardLayer: 3-17        1,050,112\n",
       "│    │    └─Dropout: 3-18                --\n",
       "│    └─TransformerDecoderLayer: 2-3      --\n",
       "│    │    └─LayerNormalization: 3-19     1,024\n",
       "│    │    └─MultiheadAttention: 3-20     1,050,624\n",
       "│    │    └─Dropout: 3-21                --\n",
       "│    │    └─LayerNormalization: 3-22     1,024\n",
       "│    │    └─MultiheadAttention: 3-23     1,050,624\n",
       "│    │    └─Dropout: 3-24                --\n",
       "│    │    └─LayerNormalization: 3-25     1,024\n",
       "│    │    └─FeedFowardLayer: 3-26        1,050,112\n",
       "│    │    └─Dropout: 3-27                --\n",
       "│    └─TransformerDecoderLayer: 2-4      --\n",
       "│    │    └─LayerNormalization: 3-28     1,024\n",
       "│    │    └─MultiheadAttention: 3-29     1,050,624\n",
       "│    │    └─Dropout: 3-30                --\n",
       "│    │    └─LayerNormalization: 3-31     1,024\n",
       "│    │    └─MultiheadAttention: 3-32     1,050,624\n",
       "│    │    └─Dropout: 3-33                --\n",
       "│    │    └─LayerNormalization: 3-34     1,024\n",
       "│    │    └─FeedFowardLayer: 3-35        1,050,112\n",
       "│    │    └─Dropout: 3-36                --\n",
       "│    └─TransformerDecoderLayer: 2-5      --\n",
       "│    │    └─LayerNormalization: 3-37     1,024\n",
       "│    │    └─MultiheadAttention: 3-38     1,050,624\n",
       "│    │    └─Dropout: 3-39                --\n",
       "│    │    └─LayerNormalization: 3-40     1,024\n",
       "│    │    └─MultiheadAttention: 3-41     1,050,624\n",
       "│    │    └─Dropout: 3-42                --\n",
       "│    │    └─LayerNormalization: 3-43     1,024\n",
       "│    │    └─FeedFowardLayer: 3-44        1,050,112\n",
       "│    │    └─Dropout: 3-45                --\n",
       "│    └─TransformerDecoderLayer: 2-6      --\n",
       "│    │    └─LayerNormalization: 3-46     1,024\n",
       "│    │    └─MultiheadAttention: 3-47     1,050,624\n",
       "│    │    └─Dropout: 3-48                --\n",
       "│    │    └─LayerNormalization: 3-49     1,024\n",
       "│    │    └─MultiheadAttention: 3-50     1,050,624\n",
       "│    │    └─Dropout: 3-51                --\n",
       "│    │    └─LayerNormalization: 3-52     1,024\n",
       "│    │    └─FeedFowardLayer: 3-53        1,050,112\n",
       "│    │    └─Dropout: 3-54                --\n",
       "├─LayerNormalization: 1-2                --\n",
       "│    └─LayerNorm: 2-7                    1,024\n",
       "=================================================================\n",
       "Total params: 18,927,616\n",
       "Trainable params: 18,927,616\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = TransformerDecoder()\n",
    "summary(model=dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab8990-184b-4854-980e-ef4887300b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f252ad6-5c3a-44ce-ab3d-4d4d75d1960a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e365900-13ff-4dd8-b789-6f45b15c0564",
   "metadata": {},
   "source": [
    "#### Action Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0bc4-3be4-4b84-a52a-7235e9af947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14293b-b031-477b-a643-3811927b1bb1",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ec749-1fbf-4072-b191-b6e3ad2f243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.transformer = TransformerDecoder()\n",
    "        # \n",
    "        self.action_generator = ActionGenerator()\n",
    "        \n",
    "\n",
    "    def _positional_encoding(\n",
    "        self,\n",
    "        seq, \n",
    "        dim, \n",
    "        temperature = 10000, \n",
    "        device = None, \n",
    "        dtype = torch.float32\n",
    "    ):\n",
    "        n = torch.arange(seq, device = device)\n",
    "        omega = torch.arange(dim // 2, device = device) / (dim // 2 - 1)\n",
    "        omega = 1. / (temperature ** omega)\n",
    "\n",
    "        n = n[:, None] * omega[None, :]\n",
    "        pos_emb = torch.cat((n.sin(), n.cos()), dim = 1)\n",
    "        \n",
    "        return pos_emb.type(dtype)\n",
    "\n",
    "    \n",
    "    def forward(self, instructions, imgs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f893fa-9324-4244-aad2-a66db2243700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cde5f94-8a3c-458d-94da-9655d7ddb2d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RT-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033aac5-7312-47d8-8ff8-8044ebb2fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RT1(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = RT1Encder()\n",
    "        self.decoder = RT1Decoder()\n",
    "        \n",
    "    def forward(self, imgs, instruction):\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd461-7028-49de-bcb9-e8464433f548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a346bbe-d174-40a9-a807-e2cd63665ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bebcb-a94a-4272-b68d-5eeee9dbd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMF-BE",
   "language": "python",
   "name": "smf-be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
