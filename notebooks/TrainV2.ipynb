{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51054b27-9fd9-4a34-9c80-9519b2fb3060",
   "metadata": {},
   "source": [
    "## ðŸ¤” Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d77a40d-8e01-420e-b572-fe22bd600f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 16 06:33:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    43W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f74170-ded5-4ca1-81b1-7b3857810090",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ff9e3c-a68b-4a84-ab94-c2cd07294dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0920d519-4636-47e0-977c-c0d6583ba014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import RichProgressBar, TQDMProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import time\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ae7b8b-5631-4c2b-a735-43e3cee5cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec61ddc-e278-454c-b6ec-538359e04c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataloader import BEDataset, BEDataModule\n",
    "from transformer import generate_causal_attention_mask\n",
    "\n",
    "from rt1 import RT1CRAM\n",
    "from utils.model_utils import plot_attention, fetch_sample_from_batch\n",
    "import utils.data_utils as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79af0e-70f8-4b36-b69a-b25466aa88b6",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df6190a-d007-4387-b470-f7ea1063f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training on 3842 samples.\n",
      "INFO:root:Validating on 666 samples.\n",
      "INFO:root:Testing on 250 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # examples: 4758\n"
     ]
    }
   ],
   "source": [
    "dm = BEDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c0210-17c3-4054-af37-3658ddc4799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_id', 'in_state', 'action_desc', 'source_mask_tokens', 'source_mask', 'motor_cmd', 'target_mask'])\n",
      "CPU times: user 1.1 s, sys: 881 ms, total: 1.98 s\n",
      "Wall time: 7.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 288, 288])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "print(batch.keys())\n",
    "batch[\"in_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad9c86c-0345-4fe7-b85b-07cdb971a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids=batch[\"action_desc\"][\"ids\"].cuda()\n",
    "# attn_mask=batch[\"action_desc\"][\"mask\"].cuda()\n",
    "# token_type_ids=batch[\"action_desc\"][\"token_type_ids\"].cuda()\n",
    "# imgs=batch[\"in_state\"].cuda()\n",
    "# decoder_inp=batch[\"motor_cmd\"][\"decoder_inp_ids\"].cuda()\n",
    "# src_mask=(batch[\"source_mask\"].cuda(), batch[\"source_mask_tokens\"].cuda())\n",
    "# target_mask=batch[\"target_mask\"].cuda()\n",
    "# labels = batch[\"motor_cmd\"][\"labels\"].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ff744-12e8-4052-9d4b-df1c92b76390",
   "metadata": {},
   "source": [
    "## ðŸ¤– RT1-CRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f735344-d5e5-4193-a4f8-bde3804622fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b3.ra2_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b3.ra2_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RT1CRAM                                                           --\n",
       "â”œâ”€RT1Encoder: 1-1                                                 --\n",
       "â”‚    â””â”€TextEncoder: 2-1                                           --\n",
       "â”‚    â”‚    â””â”€BertModel: 3-1                                        (28,763,648)\n",
       "â”‚    â”‚    â””â”€Dropout: 3-2                                          --\n",
       "â”‚    â””â”€FiLMEncoder: 2-2                                           --\n",
       "â”‚    â”‚    â””â”€ImageFeatureExtractor: 3-3                            10,300,456\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-4                                       3,170,304\n",
       "â”‚    â””â”€TokenLearnerV11: 2-3                                       --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-5                                       134,408\n",
       "â”œâ”€RT1Decoder: 1-2                                                 --\n",
       "â”‚    â””â”€Embedding: 2-4                                             26,624\n",
       "â”‚    â””â”€TransformerDecoder: 2-5                                    --\n",
       "â”‚    â”‚    â””â”€Linear: 3-6                                           262,656\n",
       "â”‚    â”‚    â””â”€Linear: 3-7                                           8,704\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-8                                       26,219,008\n",
       "â”‚    â””â”€LayerNormalization: 2-6                                    --\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-9                                        1,024\n",
       "â”‚    â””â”€ActionGenerator: 2-7                                       --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-10                                      26,676\n",
       "â”œâ”€CrossEntropyLoss: 1-3                                           --\n",
       "â”œâ”€CharErrorRate: 1-4                                              --\n",
       "â”œâ”€WordErrorRate: 1-5                                              --\n",
       "==========================================================================================\n",
       "Total params: 68,913,508\n",
       "Trainable params: 30,046,524\n",
       "Non-trainable params: 38,866,984\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt1 = RT1CRAM(\n",
    "    cnn_bacnbone=config.SELECTED_CNN_BACKBONE, \n",
    "    num_res_blocks=config.NUM_RES_BLOCKS,\n",
    "    freeze_cnn_backbone=True\n",
    ").cuda()\n",
    "# print(rt1)\n",
    "\n",
    "summary(model=rt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e608e2a-2966-4550-9526-c0983f91885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# text_enc_last_h, learned_tokens = rt1._encode(\n",
    "#     input_ids=input_ids, \n",
    "#     attn_mask=attn_mask, \n",
    "#     token_type_ids=token_type_ids, \n",
    "#     imgs=imgs    \n",
    "# )\n",
    "\n",
    "# text_enc_last_h.shape, learned_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d962c34-7c4c-40e6-b5e6-60809f411de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = rt1._decode(\n",
    "#         decoder_inp=decoder_inp, \n",
    "#         encoder_outs=(text_enc_last_h, learned_tokens), \n",
    "#         src_mask=src_mask, \n",
    "#         target_mask=target_mask\n",
    "#     )\n",
    "    \n",
    "# logits.shape, self_attn_ws.shape, cross_attn_ws_seq.shape, cross_attn_ws_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0501fba-4572-42d9-85bf-41adb6340b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_attention(attn_w=cross_attn_ws_tokens, kind=\"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aaedbf-df27-450c-a657-6754abf370ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross_attn_ws_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313f2f4-6764-4b05-b576-936f9d625fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe02812-d0e2-4a8a-b562-af007ef094be",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸â€ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9d22df-8a7b-4cba-8811-dd0bd796815a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 3e-06\n",
       "    maximize: False\n",
       "    weight_decay: 5e-06\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    ignore_index=config.TGT_PAD_TOK_ID, \n",
    "    label_smoothing=config.LABEL_SMOOTHING\n",
    ")\n",
    "\n",
    "opt = getattr(torch.optim, config.OPTIMIZER)(\n",
    "    params=[p for p in rt1.parameters() if p.requires_grad], \n",
    "    lr=config.LR,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989aec1b-f692-4e0b-99ab-9fc7c97a38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch, loss_fn):\n",
    "\n",
    "    input_ids=batch[\"action_desc\"][\"ids\"].to(config.DEVICE)\n",
    "    attn_mask=batch[\"action_desc\"][\"mask\"].to(config.DEVICE)\n",
    "    token_type_ids=batch[\"action_desc\"][\"token_type_ids\"].to(config.DEVICE)\n",
    "    imgs=batch[\"in_state\"].to(config.DEVICE)\n",
    "    decoder_inp=batch[\"motor_cmd\"][\"decoder_inp_ids\"].to(config.DEVICE)\n",
    "    src_mask=(batch[\"source_mask\"].to(config.DEVICE), batch[\"source_mask_tokens\"].to(config.DEVICE))\n",
    "    target_mask=batch[\"target_mask\"].to(config.DEVICE)\n",
    "    \n",
    "    # forward\n",
    "    logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = model(\n",
    "        input_ids=input_ids, \n",
    "        attn_mask=attn_mask, \n",
    "        token_type_ids=token_type_ids, \n",
    "        imgs=imgs,\n",
    "        decoder_inp=decoder_inp, \n",
    "        src_mask=src_mask, \n",
    "        target_mask=target_mask \n",
    "    )\n",
    "\n",
    "    # loss computation\n",
    "    labels = batch[\"motor_cmd\"][\"labels\"].to(config.DEVICE)\n",
    "    loss = loss_fn(logits.view(-1, logits.shape[2]), labels.view(-1))\n",
    "        \n",
    "    return loss, logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bbf71-5c90-44de-abaf-679aeb45b0fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ðŸ›Ÿ Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ce1de8-a9ca-494a-ad5a-5e10b26eda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(\n",
    "    model:pl.LightningModule, \n",
    "    batch_inp:dict, \n",
    "    max_len:int=config.MAX_OUT_SEQ_LEN, \n",
    "    debug:bool=False\n",
    "):\n",
    "    if model.device.type == \"cpu\":\n",
    "        model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    sos_token = config.TARGETS_MAPPING[\"[SOS]\"]\n",
    "    eos_token = config.TARGETS_MAPPING[\"[EOS]\"]\n",
    "    \n",
    "    input_ids=batch_inp[\"ids\"].to(config.DEVICE)\n",
    "    attn_mask=batch_inp[\"mask\"].to(config.DEVICE)\n",
    "    token_type_ids=batch_inp[\"token_type_ids\"].to(config.DEVICE)\n",
    "    imgs=batch_inp[\"in_state\"].to(config.DEVICE)\n",
    "    src_mask=(\n",
    "        batch_inp[\"source_mask\"].to(config.DEVICE), \n",
    "        batch_inp[\"source_mask_tokens\"].to(config.DEVICE)\n",
    "    )\n",
    "\n",
    "    text_enc_last_h, learned_tokens = model._encode(\n",
    "        input_ids=input_ids, \n",
    "        attn_mask=attn_mask, \n",
    "        token_type_ids=token_type_ids, \n",
    "        imgs=imgs    \n",
    "    )\n",
    "    \n",
    "    decoder_inp = torch.empty(1, 1, dtype=torch.long, device=input_ids.device).fill_(sos_token)\n",
    "\n",
    "    # decoding procedure\n",
    "    for t in range(max_len):\n",
    "        \n",
    "        decoder_mask = generate_causal_attention_mask(\n",
    "            dim=decoder_inp.shape[1]\n",
    "        ).type_as(attn_mask)\n",
    "        \n",
    "        # generate predictions\n",
    "        with torch.no_grad():\n",
    "            logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = model._decode(\n",
    "            decoder_inp=decoder_inp, \n",
    "            encoder_outs=(text_enc_last_h, learned_tokens), \n",
    "            src_mask=src_mask, \n",
    "            target_mask=decoder_mask,\n",
    "            debug=debug,\n",
    "            return_actions=False\n",
    "        )\n",
    "\n",
    "        # perform greedy decoding\n",
    "        probs = model.decoder.action_generator(logits[:, -1])\n",
    "            \n",
    "        _, next_tok = torch.max(probs, dim=-1)\n",
    "        # update decoder input\n",
    "        decoder_inp = torch.cat((decoder_inp, next_tok.unsqueeze(1)), dim=1)\n",
    "            \n",
    "    return decoder_inp[:, 1:].cpu().detach(), logits, self_attn_ws.cpu().detach(), cross_attn_ws_seq.cpu().detach(), cross_attn_ws_tokens.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff9aedb-5a52-48f6-b8df-d2cff2439aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(batch, model, loss_fn):\n",
    "    inp = fetch_sample_from_batch(\n",
    "        batch, \n",
    "        batch_size=batch[\"in_state\"].shape[0],\n",
    "        random=True\n",
    "    )\n",
    "    \n",
    "    pred_ids, logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = greedy_decoding(\n",
    "        model=model, \n",
    "        batch_inp=inp, \n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    labels = inp[\"labels\"].to(config.DEVICE)\n",
    "    \n",
    "    preds = model.decode_predictions(\n",
    "            predicted_ids=pred_ids\n",
    "    )[0]\n",
    "\n",
    "    label = model.decode_predictions(\n",
    "        predicted_ids=labels\n",
    "    )[0]  \n",
    "    \n",
    "    # compute metrics\n",
    "    val_loss = loss_fn(logits.view(-1, logits.shape[2]), labels.view(-1)).item()  # loss\n",
    "    cer = model.cer_fn(preds, label).item() # Character Error Rate\n",
    "    wer = model.wer_fn(preds, label).item() # Word Error Rate\n",
    "    \n",
    "    output = {\n",
    "        \"val_loss\"              : val_loss,\n",
    "        \"CER\"                   : cer,\n",
    "        \"WER\"                   : wer,\n",
    "        \"label\"                 : label,\n",
    "        \"pred_ids\"              : pred_ids,\n",
    "        \"pred_tokens\"           : preds,\n",
    "        \"self_attn_ws\"          : self_attn_ws, \n",
    "        \"cross_attn_ws_seq\"     : cross_attn_ws_seq, \n",
    "        \"cross_attn_ws_tokens\"  : cross_attn_ws_tokens\n",
    "    }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2be6db-be4c-415f-a56b-9158229cbe91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m out\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mvalidation_step\u001b[0;34m(batch, model, loss_fn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(batch, model, loss_fn):\n\u001b[1;32m      2\u001b[0m     inp \u001b[38;5;241m=\u001b[39m fetch_sample_from_batch(\n\u001b[1;32m      3\u001b[0m         batch, \n\u001b[1;32m      4\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m         random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     pred_ids, logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_inp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m     16\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_predictions(\n\u001b[1;32m     17\u001b[0m             predicted_ids\u001b[38;5;241m=\u001b[39mpred_ids\n\u001b[1;32m     18\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mgreedy_decoding\u001b[0;34m(model, batch_inp, max_len, debug)\u001b[0m\n\u001b[1;32m     17\u001b[0m imgs\u001b[38;5;241m=\u001b[39mbatch_inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m     18\u001b[0m src_mask\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m     19\u001b[0m     batch_inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE), \n\u001b[1;32m     20\u001b[0m     batch_inp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m text_enc_last_h, learned_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgs\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m decoder_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(sos_token)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# decoding procedure\u001b[39;00m\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/../src/rt1.py:304\u001b[0m, in \u001b[0;36mRT1CRAM._encode\u001b[0;34m(self, input_ids, attn_mask, token_type_ids, imgs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    298\u001b[0m     input_ids:torch\u001b[38;5;241m.\u001b[39mtensor, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     imgs:torch\u001b[38;5;241m.\u001b[39mtensor,\n\u001b[1;32m    302\u001b[0m ):\n\u001b[0;32m--> 304\u001b[0m     text_enc_last_h, learned_tokens, spatial_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgs\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_enc_last_h, learned_tokens\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/../src/rt1.py:108\u001b[0m, in \u001b[0;36mRT1Encoder.forward\u001b[0;34m(self, input_ids, attn_mask, token_type_ids, imgs)\u001b[0m\n\u001b[1;32m    101\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    102\u001b[0m     (B, config\u001b[38;5;241m.\u001b[39mNUM_HISTORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mD_MODEL, config\u001b[38;5;241m.\u001b[39mNUM_LEARNED_TOKENS),\n\u001b[1;32m    103\u001b[0m     device \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mDEVICE\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_HISTORY\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# print(history.carousel[:, :, h, :, :].shape)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     text_enc_h_state, tokens, spatial_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcarousel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_vl_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     tokenized_inputs[:,  i] \u001b[38;5;241m=\u001b[39m tokens \n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_enc_h_state, tokenized_inputs\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mD_MODEL), spatial_attn_weights\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/../src/rt1.py:78\u001b[0m, in \u001b[0;36mRT1Encoder._encode\u001b[0;34m(self, input_ids, attn_mask, token_type_ids, imgs, return_vl_tokens)\u001b[0m\n\u001b[1;32m     71\u001b[0m text_enc, text_enc_h_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(\n\u001b[1;32m     72\u001b[0m     inp_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     73\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m     74\u001b[0m     tok_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Generage vision-laguage tokens\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m vl_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilm_image_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_enc\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Extract learned tokens\u001b[39;00m\n\u001b[1;32m     84\u001b[0m learned_tokens, spatial_attn_weights  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_learner(vl_tokens)\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/../src/film_layers.py:210\u001b[0m, in \u001b[0;36mFiLMEncoder.forward\u001b[0;34m(self, x, conditioning, flatten)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, conditioning, flatten:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m             \n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# print(f\"x: {x.shape} - desc (emb): {conditioning.shape}\")\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     N, C, _, _ \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# print(f\"int. out: {out.shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/../src/utils/model_utils.py:116\u001b[0m, in \u001b[0;36mImageFeatureExtractor.forward\u001b[0;34m(self, x, flat_out, return_feats)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, flat_out:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_feats:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    114\u001b[0m     \n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# extract image features\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# return last output features\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(enc, return_feats)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flat_out:\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/timm/models/efficientnet.py:250\u001b[0m, in \u001b[0;36mEfficientNetFeatures.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/cis230036p/cmanouan/miniconda3/envs/smf_be/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = validation_step(model=rt1, batch=batch, loss_fn=loss_fn)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8dbf5-0c21-40f9-ad2f-a7756961d03e",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f24ee-c2cc-4cd3-bb87-e898490c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, dm, opt, loss_fn):\n",
    "    \n",
    "    loss_epoch = np.inf\n",
    "    val_loss = np.inf\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    cer_ = np.inf\n",
    "    wer_ = np.inf\n",
    "    \n",
    "    for e in range(config.EPOCHS):        \n",
    "        running_loss = 0.\n",
    "        num_steps = config.BATCH_SIZE + len(dm.train_dataloader())\n",
    "        \n",
    "        pbar = tqdm(\n",
    "            range(num_steps),\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "            total = num_steps\n",
    "        )\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dm.train_dataloader()):            \n",
    "            pct = 100. * step / num_steps\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {e+1}/{config.EPOCHS} - (Train {pct:.1f}%)\"\n",
    "            )\n",
    "            pbar.update()\n",
    "            \n",
    "            opt.zero_grad()\n",
    "\n",
    "            # training step\n",
    "            loss, logits, _, _, _ = training_step(model=model, batch=batch, loss_fn=loss_fn)\n",
    "            \n",
    "            running_loss += loss.item()         \n",
    "            \n",
    "            # logging\n",
    "            if step % 10 == 0:\n",
    "                pbar.set_postfix(\n",
    "                    TrainLossStep=\"{:.04f}\".format(running_loss/(step+1)),\n",
    "                    TrainLossEpoch=\"{:.04f}\".format(loss_epoch),\n",
    "                    CER=\"{:.04f}\".format(cer_),\n",
    "                    WER=\"{:.04f}\".format(wer_),\n",
    "                    ValLoss=\"{:.04f}\".format(val_loss),\n",
    "                )\n",
    "                pbar.update()\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            opt.step()\n",
    "            \n",
    "        loss_epoch = running_loss / len(dm.train_dataloader())   \n",
    "        final_lr_epoch = float(opt.param_groups[0]['lr'])\n",
    "        \n",
    "        # predictions\n",
    "        preds = logits.softmax(dim=-1).argmax(dim=-1)\n",
    "\n",
    "        # decode predictions\n",
    "        preds = model.decode_predictions(\n",
    "            predicted_ids=preds\n",
    "        )\n",
    "\n",
    "        labels = model.decode_predictions(\n",
    "            predicted_ids=batch[\"motor_cmd\"][\"labels\"]\n",
    "        )         \n",
    "            \n",
    "        # log decoded sentenses\n",
    "        with open(config.LOGGING_FILE, \"a\") as f:            \n",
    "            f.write(f\"Epoch #{e+1}\\n\")\n",
    "            f.write(f\"[Train] \\n\")\n",
    "            \n",
    "            pred = preds[0]\n",
    "            label = labels[0]\n",
    "            \n",
    "            cer_ = model.cer_fn(pred, label).item()\n",
    "            wer_ = model.wer_fn(pred, label).item()\n",
    "            f.write(f\"Predicted \\t: {pred}\\n\")\n",
    "            f.write(f\"Actual \\t\\t: {label}\\n\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # validation\n",
    "        out = validation_step(model=rt1, batch=batch, loss_fn=loss_fn)\n",
    "       \n",
    "        # plot attention weights\n",
    "        plot_attention(\n",
    "            out[\"self_attn_ws\"], \n",
    "            show=False, \n",
    "            pre_fix=\"val_selfattn\", \n",
    "            folder=\"val\",\n",
    "            epoch=e,\n",
    "            wandb_logging=True\n",
    "        )\n",
    "\n",
    "        plot_attention(\n",
    "            out[\"cross_attn_ws_seq\"],\n",
    "            kind=\"cross\", \n",
    "            pre_fix=\"val_crossattn\", \n",
    "            show=False, \n",
    "            folder=\"val\",\n",
    "            epoch=e,\n",
    "            wandb_logging=True\n",
    "        )   \n",
    "\n",
    "        plot_attention(\n",
    "            out[\"cross_attn_ws_tokens\"], \n",
    "            pre_fix=\"val_crossattn_tokens\", \n",
    "            show=False, \n",
    "            folder=\"val\",\n",
    "            epoch=e,\n",
    "            wandb_logging=True\n",
    "        )   \n",
    "        \n",
    "        # log all metrics\n",
    "        val_loss = out[\"val_loss\"]\n",
    "        # update best score\n",
    "        if val_loss < best_val_loss:\n",
    "            # save checkpoint\n",
    "            path = os.path.join(config.MODEL_PATH, \"be_model.bin\")\n",
    "            torch.save({\n",
    "                'model_state_dict'      :model.state_dict(),\n",
    "                'optimizer_state_dict'  :opt.state_dict(),\n",
    "                'val_loss'              : val_loss, \n",
    "                'epoch'                 : e\n",
    "                }, path)\n",
    "            \n",
    "            # update best score\n",
    "            best_val_loss = val_loss        \n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            TrainLossStep=\"{:.04f}\".format(running_loss/(step+1)),\n",
    "            TrainLossEpoch=\"{:.04f}\".format(loss_epoch),\n",
    "            # CER=\"{:.04f}\".format(cer_),\n",
    "            # WER=\"{:.04f}\".format(wer_),\n",
    "            ValLoss=\"{:.04f}\".format(val_loss),\n",
    "            ValCER=\"{:.04f}\".format(out[\"CER\"]),\n",
    "            ValWER=\"{:.04f}\".format(out[\"WER\"]),\n",
    "            lr=\"{:.1e}\".format(final_lr_epoch),\n",
    "        )  \n",
    "        pbar.update()\n",
    "        \n",
    "        logs_dict = dict(\n",
    "            TrainLossStep=\"{:.04f}\".format(running_loss/(step+1)),\n",
    "            TrainLossEpoch=\"{:.04f}\".format(loss_epoch),\n",
    "            ValLoss=\"{:.04f}\".format(val_loss),\n",
    "            ValCER=\"{:.04f}\".format(out[\"CER\"]),\n",
    "            ValWER=\"{:.04f}\".format(out[\"WER\"]),\n",
    "            lr=\"{:.1e}\".format(final_lr_epoch),\n",
    "        )\n",
    "        wandb.log(logs_dict)\n",
    "        \n",
    "        # log decoded sentenses\n",
    "        with open(config.LOGGING_FILE, \"a\") as f:                        \n",
    "            pred = out[\"pred_tokens\"]\n",
    "            label = out[\"label\"]\n",
    "            \n",
    "            f.write(f\"[Val] \\n\")            \n",
    "            f.write(f\"Predicted \\t: {pred}\\n\")\n",
    "            f.write(f\"Actual \\t\\t: {label}\\n\") \n",
    "            f.write(f\"Curr val loss \\t\\t: {val_loss:.5f}\\n\") \n",
    "            f.write(f\"Best loss: \\t\\t: {best_val_loss:.5f}\\n\\n\") \n",
    "            \n",
    "        pbar.close()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72979ca-9b33-4af9-b772-9cf39cccb6d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init experiment\n",
    "wandb.init(\n",
    "    project='SMF-Be', \n",
    "    group=\"RT1-CRAM\", \n",
    "    name=\"be_model\", \n",
    "    reinit=True\n",
    ")\n",
    "with open(config.LOGGING_FILE, \"a\") as f:   \n",
    "    f.write(\"*** New experiment ***\\n\")\n",
    "    \n",
    "    \n",
    "trained_model = run_experiment(\n",
    "    model=rt1, \n",
    "    dm=dm, \n",
    "    opt=opt, \n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa000104-cc3b-4927-ae6e-b7ec8745e400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2c917-08ae-444c-a456-fd3ace430e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed396280-b161-4b1f-b8b6-0801e3786cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0a9f9-42d6-4905-bc84-c23b468c7c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246745c7-2958-493c-90b9-94e77b0b85b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMF-BE",
   "language": "python",
   "name": "smf-be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
