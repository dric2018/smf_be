{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51054b27-9fd9-4a34-9c80-9519b2fb3060",
   "metadata": {},
   "source": [
    "## ðŸ¤” Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d77a40d-8e01-420e-b572-fe22bd600f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 18 02:31:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    73W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f74170-ded5-4ca1-81b1-7b3857810090",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ðŸ› ï¸ Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ff9e3c-a68b-4a84-ab94-c2cd07294dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0920d519-4636-47e0-977c-c0d6583ba014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import RichProgressBar, TQDMProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import time\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ae7b8b-5631-4c2b-a735-43e3cee5cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec61ddc-e278-454c-b6ec-538359e04c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataloader import BEDataset, BEDataModule\n",
    "from transformer import generate_causal_attention_mask\n",
    "\n",
    "from rt1 import RT1CRAM\n",
    "from utils.model_utils import plot_attention, fetch_sample_from_batch\n",
    "import utils.data_utils as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79af0e-70f8-4b36-b69a-b25466aa88b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ðŸ“Š Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df6190a-d007-4387-b470-f7ea1063f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training on 4054 samples.\n",
      "INFO:root:Validating on 454 samples.\n",
      "INFO:root:Testing on 250 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # examples: 4758\n"
     ]
    }
   ],
   "source": [
    "dm = BEDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c0210-17c3-4054-af37-3658ddc4799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sample_id', 'in_state', 'action_desc', 'source_mask_tokens', 'source_mask', 'motor_cmd', 'target_mask'])\n",
      "CPU times: user 1.29 s, sys: 1.25 s, total: 2.54 s\n",
      "Wall time: 33.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 288, 288])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "print(batch.keys())\n",
    "batch[\"in_state\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ff744-12e8-4052-9d4b-df1c92b76390",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ¤– RT1-CRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f735344-d5e5-4193-a4f8-bde3804622fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b3.ra2_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b3.ra2_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RT1CRAM                                                           --\n",
       "â”œâ”€RT1Encoder: 1-1                                                 --\n",
       "â”‚    â””â”€TextEncoder: 2-1                                           --\n",
       "â”‚    â”‚    â””â”€BertModel: 3-1                                        (4,385,920)\n",
       "â”‚    â”‚    â””â”€Dropout: 3-2                                          --\n",
       "â”‚    â””â”€FiLMEncoder: 2-2                                           --\n",
       "â”‚    â”‚    â””â”€ImageFeatureExtractor: 3-3                            10,152,616\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-4                                       135,168\n",
       "â”‚    â””â”€TokenLearnerV11: 2-3                                       --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-5                                       35,336\n",
       "â”œâ”€RT1Decoder: 1-2                                                 --\n",
       "â”‚    â””â”€Embedding: 2-4                                             6,656\n",
       "â”‚    â””â”€TransformerDecoder: 2-5                                    --\n",
       "â”‚    â”‚    â””â”€Linear: 3-6                                           16,512\n",
       "â”‚    â”‚    â””â”€Linear: 3-7                                           2,176\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-8                                       4,201,984\n",
       "â”‚    â””â”€LayerNormalization: 2-6                                    --\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-9                                        256\n",
       "â”‚    â””â”€ActionGenerator: 2-7                                       --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-10                                      6,708\n",
       "â”œâ”€CrossEntropyLoss: 1-3                                           --\n",
       "â”œâ”€CharErrorRate: 1-4                                              --\n",
       "â”œâ”€WordErrorRate: 1-5                                              --\n",
       "==========================================================================================\n",
       "Total params: 18,943,332\n",
       "Trainable params: 4,454,076\n",
       "Non-trainable params: 14,489,256\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt1 = RT1CRAM(\n",
    "    cnn_bacnbone=config.SELECTED_CNN_BACKBONE, \n",
    "    num_res_blocks=config.NUM_RES_BLOCKS,\n",
    "    freeze_cnn_backbone=config.FREEZE_CNN\n",
    ").cuda()\n",
    "# print(rt1)\n",
    "\n",
    "summary(model=rt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe02812-d0e2-4a8a-b562-af007ef094be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ðŸ‹ï¸â€ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9d22df-8a7b-4cba-8811-dd0bd796815a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 1e-06\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    ignore_index=config.TGT_PAD_TOK_ID, \n",
    "    label_smoothing=config.LABEL_SMOOTHING\n",
    ")\n",
    "\n",
    "opt = getattr(torch.optim, config.OPTIMIZER)(\n",
    "    params=[p for p in rt1.parameters() if p.requires_grad], \n",
    "    lr=config.LR,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = getattr(torch.optim.lr_scheduler, config.LR_SCHEDULER[\"type\"])(**config.LR_SCHEDULER[\"params\"], optimizer=opt)\n",
    "\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989aec1b-f692-4e0b-99ab-9fc7c97a38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch, loss_fn):\n",
    "\n",
    "    input_ids=batch[\"action_desc\"][\"ids\"].to(config.DEVICE)\n",
    "    attn_mask=batch[\"action_desc\"][\"mask\"].to(config.DEVICE)\n",
    "    token_type_ids=batch[\"action_desc\"][\"token_type_ids\"].to(config.DEVICE)\n",
    "    imgs=batch[\"in_state\"].to(config.DEVICE)\n",
    "    decoder_inp=batch[\"motor_cmd\"][\"decoder_inp_ids\"].to(config.DEVICE)\n",
    "    src_mask=(batch[\"source_mask\"].to(config.DEVICE), batch[\"source_mask_tokens\"].to(config.DEVICE))\n",
    "    target_mask=batch[\"target_mask\"].to(config.DEVICE)\n",
    "    \n",
    "    # forward\n",
    "    logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = model(\n",
    "        input_ids=input_ids, \n",
    "        attn_mask=attn_mask, \n",
    "        token_type_ids=token_type_ids, \n",
    "        imgs=imgs,\n",
    "        decoder_inp=decoder_inp, \n",
    "        src_mask=src_mask, \n",
    "        target_mask=target_mask \n",
    "    )\n",
    "\n",
    "    # loss computation\n",
    "    labels = batch[\"motor_cmd\"][\"labels\"].to(config.DEVICE)\n",
    "    loss = loss_fn(logits.view(-1, logits.shape[2]), labels.view(-1))\n",
    "        \n",
    "    return loss, logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bbf71-5c90-44de-abaf-679aeb45b0fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ðŸ›Ÿ Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ce1de8-a9ca-494a-ad5a-5e10b26eda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(\n",
    "    model:pl.LightningModule, \n",
    "    batch_inp:dict, \n",
    "    max_len:int=config.MAX_OUT_SEQ_LEN, \n",
    "    debug:bool=False\n",
    "):\n",
    "    if model.device.type == \"cpu\":\n",
    "        model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    sos_token = config.TARGETS_MAPPING[\"[SOS]\"]\n",
    "    eos_token = config.TARGETS_MAPPING[\"[EOS]\"]\n",
    "    \n",
    "    input_ids=batch_inp[\"ids\"].to(config.DEVICE)\n",
    "    attn_mask=batch_inp[\"mask\"].to(config.DEVICE)\n",
    "    token_type_ids=batch_inp[\"token_type_ids\"].to(config.DEVICE)\n",
    "    imgs=batch_inp[\"in_state\"].to(config.DEVICE)\n",
    "    src_mask=(\n",
    "        batch_inp[\"source_mask\"].to(config.DEVICE), \n",
    "        batch_inp[\"source_mask_tokens\"].to(config.DEVICE)\n",
    "    )\n",
    "\n",
    "    text_enc_last_h, learned_tokens = model._encode(\n",
    "        input_ids=input_ids, \n",
    "        attn_mask=attn_mask, \n",
    "        token_type_ids=token_type_ids, \n",
    "        imgs=imgs    \n",
    "    )\n",
    "    \n",
    "    decoder_inp = torch.empty(1, 1, dtype=torch.long, device=input_ids.device).fill_(sos_token)\n",
    "\n",
    "    # decoding procedure\n",
    "    for t in range(max_len):\n",
    "        \n",
    "        decoder_mask = generate_causal_attention_mask(\n",
    "            dim=decoder_inp.shape[1]\n",
    "        ).type_as(attn_mask)\n",
    "        \n",
    "        # generate predictions\n",
    "        with torch.no_grad():\n",
    "            logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = model._decode(\n",
    "            decoder_inp=decoder_inp, \n",
    "            encoder_outs=(text_enc_last_h, learned_tokens), \n",
    "            src_mask=src_mask, \n",
    "            target_mask=decoder_mask,\n",
    "            debug=debug,\n",
    "            return_actions=False\n",
    "        )\n",
    "\n",
    "        # perform greedy decoding\n",
    "        probs = model.decoder.action_generator(logits[:, -1])\n",
    "            \n",
    "        _, next_tok = torch.max(probs, dim=-1)\n",
    "        # update decoder input\n",
    "        decoder_inp = torch.cat((decoder_inp, next_tok.unsqueeze(1)), dim=1)\n",
    "            \n",
    "    return decoder_inp[:, 1:].cpu().detach(), logits, self_attn_ws.cpu().detach(), cross_attn_ws_seq.cpu().detach(), cross_attn_ws_tokens.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff9aedb-5a52-48f6-b8df-d2cff2439aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(batch, model, loss_fn, debug:bool=False):\n",
    "    inp = fetch_sample_from_batch(\n",
    "        batch, \n",
    "        batch_size=batch[\"in_state\"].shape[0],\n",
    "        random=True\n",
    "    )\n",
    "    \n",
    "    pred_ids, logits, self_attn_ws, cross_attn_ws_seq, cross_attn_ws_tokens = greedy_decoding(\n",
    "        model=model, \n",
    "        batch_inp=inp, \n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    labels = inp[\"labels\"].to(config.DEVICE)\n",
    "    \n",
    "    preds = model.decode_predictions(\n",
    "            predicted_ids=pred_ids\n",
    "    )[0]\n",
    "\n",
    "    label = model.decode_predictions(\n",
    "        predicted_ids=labels\n",
    "    )[0]  \n",
    "    \n",
    "    # compute metrics\n",
    "    val_loss = loss_fn(logits.view(-1, logits.shape[2]), labels.view(-1)).item()  # loss\n",
    "    cer = model.cer_fn(preds, label).item() # Character Error Rate\n",
    "    wer = model.wer_fn(preds, label).item() # Word Error Rate\n",
    "    \n",
    "    output = {\n",
    "        \"val_loss\"              : val_loss,\n",
    "        \"CER\"                   : cer,\n",
    "        \"WER\"                   : wer,\n",
    "        \"label\"                 : label,\n",
    "        \"pred_ids\"              : pred_ids,\n",
    "        \"pred_tokens\"           : preds,\n",
    "        \"self_attn_ws\"          : self_attn_ws, \n",
    "        \"cross_attn_ws_seq\"     : cross_attn_ws_seq, \n",
    "        \"cross_attn_ws_tokens\"  : cross_attn_ws_tokens\n",
    "    }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2be6db-be4c-415f-a56b-9158229cbe91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 5.0303168296813965,\n",
       " 'CER': 0.9259259104728699,\n",
       " 'WER': 1.5555555820465088,\n",
       " 'label': \":BOTTLE GREEN POSE-9 :GLOVE RED POSE-5 :BOTTLE #'*backward-transformation* :GLOVE\",\n",
       " 'pred_ids': tensor([[50, 50, 50, 50, 50, 50, 50, 50, 50, 50,  2, 36, 15,  2, 36, 29]]),\n",
       " 'pred_tokens': 'POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-7 POSE-3 :BUTTERMILK POSE-3 :POT',\n",
       " 'self_attn_ws': tensor([[[[[8.9896e-37, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             1.0000e+00, 3.9739e-22],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            ...,\n",
       "            [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1383e-18,\n",
       "             4.4762e-18, 1.4013e-45],\n",
       "            [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 5.1706e-16],\n",
       "            [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "           [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             2.6642e-20, 0.0000e+00],\n",
       "            [0.0000e+00, 5.2170e-29, 1.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 3.7203e-31, 1.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            ...,\n",
       "            [0.0000e+00, 2.5265e-42, 0.0000e+00,  ..., 1.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [9.9223e-23, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             5.6812e-12, 0.0000e+00],\n",
       "            [2.3778e-29, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             1.0000e+00, 0.0000e+00]],\n",
       " \n",
       "           [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 3.3407e-21, 6.5720e-09,  ..., 0.0000e+00,\n",
       "             2.3668e-14, 0.0000e+00],\n",
       "            [0.0000e+00, 3.9579e-37, 7.6752e-24,  ..., 1.2078e-16,\n",
       "             9.7380e-01, 0.0000e+00],\n",
       "            ...,\n",
       "            [0.0000e+00, 1.1155e-18, 4.3290e-18,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 1.3365e-11, 1.8209e-25,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 1.8831e-31],\n",
       "            [5.3130e-24, 0.0000e+00, 0.0000e+00,  ..., 1.4476e-29,\n",
       "             6.5132e-25, 1.0000e+00]],\n",
       " \n",
       "           [[2.9994e-08, 6.0509e-33, 4.1811e-18,  ..., 1.4857e-22,\n",
       "             9.5327e-01, 0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 2.3138e-17],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 2.7475e-15],\n",
       "            ...,\n",
       "            [3.0201e-02, 0.0000e+00, 0.0000e+00,  ..., 1.4013e-45,\n",
       "             0.0000e+00, 1.7925e-02],\n",
       "            [0.0000e+00, 1.4677e-19, 1.2374e-02,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 1.0845e-14, 7.7891e-12,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "          [[[3.2307e-02, 7.9086e-02, 2.9486e-02,  ..., 1.0316e-01,\n",
       "             1.6410e-01, 1.7134e-02],\n",
       "            [1.0256e-01, 5.4952e-02, 6.5726e-02,  ..., 2.7377e-02,\n",
       "             9.9299e-02, 2.3189e-02],\n",
       "            [2.1220e-01, 4.2426e-02, 3.1327e-02,  ..., 8.9786e-02,\n",
       "             1.3074e-01, 2.2743e-02],\n",
       "            ...,\n",
       "            [8.4741e-02, 5.2685e-02, 6.8526e-02,  ..., 7.0372e-02,\n",
       "             4.6791e-02, 5.5103e-02],\n",
       "            [2.3195e-02, 6.8998e-02, 9.7620e-02,  ..., 1.5348e-02,\n",
       "             3.8227e-02, 1.0571e-02],\n",
       "            [8.4983e-03, 1.1759e-01, 6.2081e-02,  ..., 1.0745e-01,\n",
       "             1.5949e-02, 4.9702e-03]],\n",
       " \n",
       "           [[1.7569e-02, 9.6929e-02, 6.9828e-02,  ..., 1.9360e-02,\n",
       "             7.3683e-02, 1.0480e-02],\n",
       "            [1.5166e-02, 4.9609e-02, 5.8241e-02,  ..., 1.9243e-02,\n",
       "             1.0917e-01, 1.2218e-01],\n",
       "            [1.1469e-02, 4.3247e-02, 5.2201e-02,  ..., 3.2677e-02,\n",
       "             6.2898e-02, 2.1666e-01],\n",
       "            ...,\n",
       "            [3.2878e-03, 8.9800e-02, 1.0706e-01,  ..., 1.0915e-02,\n",
       "             5.6958e-03, 1.6091e-03],\n",
       "            [4.4895e-01, 2.7713e-02, 5.7533e-02,  ..., 1.8281e-02,\n",
       "             2.1822e-02, 1.7246e-02],\n",
       "            [1.0301e-02, 7.8636e-02, 1.0287e-01,  ..., 4.8745e-03,\n",
       "             7.1262e-03, 2.2648e-03]],\n",
       " \n",
       "           [[3.7765e-02, 4.9180e-02, 3.2951e-02,  ..., 1.5384e-01,\n",
       "             8.0302e-02, 7.9443e-02],\n",
       "            [6.7280e-02, 7.1896e-02, 7.5121e-02,  ..., 6.2090e-03,\n",
       "             2.0971e-02, 7.5641e-02],\n",
       "            [5.4143e-02, 8.8597e-02, 7.2778e-02,  ..., 9.3968e-03,\n",
       "             2.2479e-02, 5.4721e-02],\n",
       "            ...,\n",
       "            [4.1075e-02, 8.1878e-02, 6.7973e-02,  ..., 2.1001e-02,\n",
       "             3.0417e-02, 3.7374e-02],\n",
       "            [1.3116e-01, 5.5592e-02, 4.8576e-02,  ..., 2.5895e-02,\n",
       "             2.7112e-02, 1.5118e-01],\n",
       "            [4.7209e-02, 7.9414e-02, 9.5899e-02,  ..., 5.9355e-03,\n",
       "             5.8925e-03, 2.1133e-02]],\n",
       " \n",
       "           [[3.7062e-02, 8.2151e-02, 6.1517e-02,  ..., 4.2908e-02,\n",
       "             7.6318e-02, 4.2065e-02],\n",
       "            [1.0672e-01, 4.3819e-02, 7.6545e-02,  ..., 2.6900e-02,\n",
       "             3.2063e-02, 7.9541e-02],\n",
       "            [5.5290e-02, 5.5174e-02, 1.0468e-01,  ..., 1.2889e-02,\n",
       "             1.9562e-02, 2.8081e-02],\n",
       "            ...,\n",
       "            [1.3945e-01, 3.0329e-02, 3.8713e-02,  ..., 2.9696e-02,\n",
       "             2.1284e-02, 1.7201e-01],\n",
       "            [7.2417e-02, 7.5938e-02, 6.8395e-02,  ..., 1.4200e-02,\n",
       "             4.5066e-02, 5.6112e-02],\n",
       "            [1.4802e-01, 5.7148e-02, 5.3263e-02,  ..., 4.1312e-02,\n",
       "             8.0734e-02, 5.2151e-02]]],\n",
       " \n",
       " \n",
       "          [[[7.9790e-02, 5.1833e-02, 6.7295e-02,  ..., 1.1671e-01,\n",
       "             3.7846e-02, 4.5881e-02],\n",
       "            [8.1052e-02, 4.5930e-02, 4.5768e-02,  ..., 3.8088e-02,\n",
       "             1.0101e-01, 9.2844e-02],\n",
       "            [5.2959e-02, 4.6030e-02, 6.6335e-02,  ..., 2.5690e-02,\n",
       "             5.2245e-02, 9.3846e-02],\n",
       "            ...,\n",
       "            [1.1248e-01, 4.4694e-02, 2.6457e-02,  ..., 4.1226e-02,\n",
       "             1.5900e-01, 7.0450e-02],\n",
       "            [4.4476e-02, 3.8046e-02, 3.7627e-02,  ..., 2.2071e-01,\n",
       "             9.3955e-02, 7.4608e-02],\n",
       "            [9.3463e-02, 5.6139e-02, 6.9783e-02,  ..., 4.3197e-02,\n",
       "             5.0721e-02, 4.1202e-02]],\n",
       " \n",
       "           [[9.4070e-02, 3.4615e-02, 7.4475e-02,  ..., 3.5750e-02,\n",
       "             7.7384e-02, 3.7496e-02],\n",
       "            [1.1120e-01, 3.6054e-02, 8.8131e-02,  ..., 6.2818e-03,\n",
       "             3.9197e-02, 2.4527e-02],\n",
       "            [2.0864e-01, 2.6937e-02, 7.2871e-02,  ..., 8.6963e-03,\n",
       "             3.8611e-02, 3.4983e-02],\n",
       "            ...,\n",
       "            [8.0703e-02, 6.6899e-02, 6.2077e-02,  ..., 1.6762e-02,\n",
       "             6.9484e-02, 5.9020e-02],\n",
       "            [1.5509e-01, 4.2148e-02, 5.6412e-02,  ..., 1.3639e-02,\n",
       "             1.1874e-01, 2.2193e-02],\n",
       "            [1.0616e-01, 4.2320e-02, 3.4034e-02,  ..., 1.9981e-02,\n",
       "             1.8117e-01, 6.0930e-02]],\n",
       " \n",
       "           [[1.4832e-01, 3.0766e-02, 4.7169e-02,  ..., 1.4114e-02,\n",
       "             2.6068e-02, 1.8772e-01],\n",
       "            [1.1623e-01, 3.8170e-02, 3.8056e-02,  ..., 1.5771e-01,\n",
       "             9.3668e-02, 6.4634e-02],\n",
       "            [8.6683e-02, 5.6567e-02, 5.4253e-02,  ..., 1.5467e-01,\n",
       "             5.7190e-02, 3.6715e-02],\n",
       "            ...,\n",
       "            [1.4332e-01, 1.7931e-02, 1.2525e-02,  ..., 3.9681e-02,\n",
       "             5.7769e-02, 3.0803e-01],\n",
       "            [1.0625e-01, 3.0593e-02, 2.9481e-02,  ..., 1.1585e-01,\n",
       "             7.1930e-02, 1.3418e-01],\n",
       "            [2.5822e-02, 7.0184e-02, 5.3478e-02,  ..., 5.8052e-02,\n",
       "             2.8399e-02, 1.4830e-01]],\n",
       " \n",
       "           [[1.2232e-01, 7.1928e-02, 4.4832e-02,  ..., 5.9005e-02,\n",
       "             5.0498e-02, 5.7506e-02],\n",
       "            [9.9309e-02, 7.0172e-02, 6.8736e-02,  ..., 3.9492e-02,\n",
       "             2.0649e-02, 5.3698e-02],\n",
       "            [5.2630e-02, 8.0195e-02, 7.9391e-02,  ..., 3.0078e-02,\n",
       "             1.6832e-02, 3.8315e-02],\n",
       "            ...,\n",
       "            [4.9087e-01, 1.7506e-02, 3.6916e-02,  ..., 1.5887e-02,\n",
       "             3.3455e-02, 5.6910e-02],\n",
       "            [2.9459e-01, 2.4502e-02, 2.5360e-02,  ..., 2.6484e-02,\n",
       "             1.0859e-01, 1.0926e-01],\n",
       "            [1.8373e-01, 4.3131e-02, 4.4010e-02,  ..., 5.3120e-02,\n",
       "             1.0894e-01, 4.4067e-02]]],\n",
       " \n",
       " \n",
       "          [[[1.2274e-01, 3.4514e-02, 8.4708e-02,  ..., 1.1254e-02,\n",
       "             4.3623e-02, 3.8283e-02],\n",
       "            [8.2833e-02, 4.0748e-02, 7.9529e-02,  ..., 3.6871e-02,\n",
       "             5.5298e-02, 4.3746e-02],\n",
       "            [1.5161e-01, 3.7793e-02, 6.9094e-02,  ..., 3.0325e-02,\n",
       "             5.7351e-02, 5.0377e-02],\n",
       "            ...,\n",
       "            [1.0346e-01, 3.7541e-02, 7.1752e-02,  ..., 3.9816e-02,\n",
       "             4.5972e-02, 4.6291e-02],\n",
       "            [6.3614e-02, 3.8503e-02, 7.4525e-02,  ..., 3.5514e-02,\n",
       "             4.9465e-02, 5.1088e-02],\n",
       "            [8.6829e-02, 4.6541e-02, 7.7493e-02,  ..., 4.9645e-02,\n",
       "             3.6973e-02, 2.6476e-02]],\n",
       " \n",
       "           [[3.1122e-02, 7.8204e-02, 7.3433e-02,  ..., 2.6632e-02,\n",
       "             5.8770e-02, 5.6893e-02],\n",
       "            [8.2222e-02, 5.5597e-02, 6.7617e-02,  ..., 1.1977e-01,\n",
       "             4.1233e-02, 4.7066e-02],\n",
       "            [6.3929e-02, 6.1862e-02, 7.5685e-02,  ..., 8.8134e-02,\n",
       "             3.6082e-02, 3.8412e-02],\n",
       "            ...,\n",
       "            [5.8840e-02, 7.5138e-02, 8.3900e-02,  ..., 6.4421e-02,\n",
       "             2.0206e-02, 2.8290e-02],\n",
       "            [1.4235e-01, 7.0310e-02, 7.2401e-02,  ..., 7.5538e-02,\n",
       "             1.8075e-02, 3.3197e-02],\n",
       "            [1.0980e-01, 8.1402e-02, 7.3924e-02,  ..., 6.3683e-02,\n",
       "             1.8227e-02, 1.8457e-02]],\n",
       " \n",
       "           [[3.9584e-02, 6.6371e-02, 7.8748e-02,  ..., 2.3206e-02,\n",
       "             6.4670e-02, 5.7184e-02],\n",
       "            [3.6922e-01, 3.0625e-02, 4.0159e-02,  ..., 1.7881e-02,\n",
       "             4.9315e-02, 6.1624e-02],\n",
       "            [3.5306e-01, 2.8100e-02, 3.9052e-02,  ..., 1.5686e-02,\n",
       "             6.5185e-02, 7.1474e-02],\n",
       "            ...,\n",
       "            [1.9043e-01, 5.3482e-02, 4.7670e-02,  ..., 3.2957e-02,\n",
       "             4.9745e-02, 7.5097e-02],\n",
       "            [2.6327e-01, 3.1427e-02, 3.7034e-02,  ..., 3.4037e-02,\n",
       "             8.7340e-02, 6.2847e-02],\n",
       "            [5.6675e-02, 5.6782e-02, 5.8752e-02,  ..., 3.2578e-02,\n",
       "             6.6901e-02, 1.0856e-01]],\n",
       " \n",
       "           [[9.7033e-02, 5.7473e-02, 6.2912e-02,  ..., 9.2510e-02,\n",
       "             4.4508e-02, 6.1916e-02],\n",
       "            [2.8570e-02, 9.8442e-02, 6.7701e-02,  ..., 4.0777e-02,\n",
       "             2.1526e-02, 3.6705e-02],\n",
       "            [3.8425e-02, 9.6325e-02, 6.5700e-02,  ..., 3.8661e-02,\n",
       "             2.6688e-02, 3.6568e-02],\n",
       "            ...,\n",
       "            [1.9259e-02, 1.0309e-01, 7.3840e-02,  ..., 3.0050e-02,\n",
       "             1.1182e-02, 2.6171e-02],\n",
       "            [2.8504e-02, 7.7814e-02, 7.0691e-02,  ..., 6.9161e-02,\n",
       "             5.1829e-02, 2.9635e-02],\n",
       "            [3.6701e-02, 9.1793e-02, 7.6364e-02,  ..., 2.9972e-02,\n",
       "             2.5291e-02, 1.7559e-02]]]]]),\n",
       " 'cross_attn_ws_seq': tensor([[[[[0.0479, 0.0435, 0.0666,  ..., 0.0913, 0.0863, 0.0636],\n",
       "            [0.0480, 0.0338, 0.0627,  ..., 0.0673, 0.0501, 0.0705],\n",
       "            [0.0360, 0.0268, 0.0479,  ..., 0.0605, 0.0531, 0.0737],\n",
       "            ...,\n",
       "            [0.0468, 0.0471, 0.1025,  ..., 0.0758, 0.0398, 0.0452],\n",
       "            [0.0386, 0.1235, 0.0456,  ..., 0.0565, 0.0436, 0.0499],\n",
       "            [0.0911, 0.0968, 0.0349,  ..., 0.0533, 0.0468, 0.0343]],\n",
       " \n",
       "           [[0.0898, 0.0607, 0.0730,  ..., 0.0778, 0.0770, 0.0598],\n",
       "            [0.0387, 0.0749, 0.0817,  ..., 0.0621, 0.0555, 0.0618],\n",
       "            [0.0586, 0.0781, 0.0831,  ..., 0.0474, 0.0666, 0.0841],\n",
       "            ...,\n",
       "            [0.0282, 0.0813, 0.0932,  ..., 0.0614, 0.0604, 0.0578],\n",
       "            [0.0210, 0.0424, 0.0612,  ..., 0.0551, 0.0611, 0.0699],\n",
       "            [0.0930, 0.0410, 0.0969,  ..., 0.0355, 0.0384, 0.0648]],\n",
       " \n",
       "           [[0.0791, 0.0462, 0.0363,  ..., 0.0715, 0.0657, 0.0523],\n",
       "            [0.0572, 0.0253, 0.0992,  ..., 0.0520, 0.0360, 0.0405],\n",
       "            [0.0716, 0.0228, 0.1112,  ..., 0.0548, 0.0331, 0.0357],\n",
       "            ...,\n",
       "            [0.0548, 0.0656, 0.1252,  ..., 0.0302, 0.0339, 0.0458],\n",
       "            [0.0225, 0.1086, 0.0437,  ..., 0.0638, 0.0662, 0.0595],\n",
       "            [0.0412, 0.0601, 0.0378,  ..., 0.0461, 0.0544, 0.0692]],\n",
       " \n",
       "           [[0.0524, 0.1041, 0.0645,  ..., 0.0264, 0.0336, 0.0340],\n",
       "            [0.1170, 0.0168, 0.0691,  ..., 0.0558, 0.0500, 0.0703],\n",
       "            [0.0933, 0.0213, 0.0797,  ..., 0.0609, 0.0565, 0.0724],\n",
       "            ...,\n",
       "            [0.0543, 0.0443, 0.0818,  ..., 0.1135, 0.0845, 0.0873],\n",
       "            [0.0319, 0.0994, 0.0802,  ..., 0.0513, 0.0538, 0.0630],\n",
       "            [0.0263, 0.0441, 0.0467,  ..., 0.0640, 0.0505, 0.0365]]],\n",
       " \n",
       " \n",
       "          [[[0.0403, 0.0540, 0.1365,  ..., 0.0452, 0.0355, 0.0508],\n",
       "            [0.0359, 0.0445, 0.0991,  ..., 0.0314, 0.0265, 0.0408],\n",
       "            [0.0422, 0.0364, 0.1005,  ..., 0.0346, 0.0294, 0.0444],\n",
       "            ...,\n",
       "            [0.0292, 0.0600, 0.1642,  ..., 0.0270, 0.0348, 0.0634],\n",
       "            [0.0317, 0.0543, 0.0767,  ..., 0.0487, 0.0451, 0.0727],\n",
       "            [0.0655, 0.0408, 0.0880,  ..., 0.0474, 0.0458, 0.0627]],\n",
       " \n",
       "           [[0.1039, 0.0356, 0.0346,  ..., 0.0898, 0.0951, 0.0922],\n",
       "            [0.1021, 0.0195, 0.0199,  ..., 0.0887, 0.0501, 0.0512],\n",
       "            [0.1365, 0.0201, 0.0196,  ..., 0.0746, 0.0319, 0.0347],\n",
       "            ...,\n",
       "            [0.0649, 0.0244, 0.0174,  ..., 0.0704, 0.0384, 0.0361],\n",
       "            [0.0294, 0.0195, 0.0244,  ..., 0.0852, 0.1232, 0.1025],\n",
       "            [0.0601, 0.0342, 0.0252,  ..., 0.0593, 0.1016, 0.1065]],\n",
       " \n",
       "           [[0.0577, 0.0418, 0.0413,  ..., 0.1187, 0.0913, 0.0598],\n",
       "            [0.0644, 0.0394, 0.0447,  ..., 0.0685, 0.0511, 0.0694],\n",
       "            [0.0634, 0.0351, 0.0524,  ..., 0.0628, 0.0463, 0.0623],\n",
       "            ...,\n",
       "            [0.0723, 0.0996, 0.0546,  ..., 0.0788, 0.0484, 0.0508],\n",
       "            [0.0249, 0.0640, 0.0314,  ..., 0.1102, 0.0611, 0.0538],\n",
       "            [0.0580, 0.0755, 0.0728,  ..., 0.1262, 0.0569, 0.0468]],\n",
       " \n",
       "           [[0.0244, 0.0204, 0.0328,  ..., 0.0445, 0.0336, 0.0274],\n",
       "            [0.0398, 0.0585, 0.0392,  ..., 0.0552, 0.0384, 0.0336],\n",
       "            [0.0379, 0.0864, 0.0370,  ..., 0.0700, 0.0569, 0.0450],\n",
       "            ...,\n",
       "            [0.0440, 0.0640, 0.1093,  ..., 0.0892, 0.0359, 0.0322],\n",
       "            [0.0364, 0.0438, 0.0429,  ..., 0.0283, 0.0261, 0.0253],\n",
       "            [0.0317, 0.0215, 0.0660,  ..., 0.0585, 0.0233, 0.0228]]],\n",
       " \n",
       " \n",
       "          [[[0.0277, 0.1073, 0.1696,  ..., 0.0428, 0.0332, 0.0416],\n",
       "            [0.0683, 0.0995, 0.2196,  ..., 0.0297, 0.0309, 0.0488],\n",
       "            [0.0493, 0.0866, 0.2187,  ..., 0.0415, 0.0333, 0.0434],\n",
       "            ...,\n",
       "            [0.0635, 0.1001, 0.1314,  ..., 0.0347, 0.0386, 0.0537],\n",
       "            [0.0701, 0.1058, 0.1296,  ..., 0.0249, 0.0342, 0.0509],\n",
       "            [0.0792, 0.1017, 0.0946,  ..., 0.0351, 0.0411, 0.0521]],\n",
       " \n",
       "           [[0.0701, 0.0332, 0.0919,  ..., 0.0369, 0.0362, 0.0462],\n",
       "            [0.0558, 0.0950, 0.1080,  ..., 0.0329, 0.0543, 0.0652],\n",
       "            [0.0560, 0.0759, 0.1050,  ..., 0.0330, 0.0431, 0.0532],\n",
       "            ...,\n",
       "            [0.0580, 0.0742, 0.0524,  ..., 0.0531, 0.0864, 0.0935],\n",
       "            [0.1162, 0.0618, 0.0412,  ..., 0.0467, 0.0688, 0.0614],\n",
       "            [0.1228, 0.0956, 0.0663,  ..., 0.0528, 0.0544, 0.0555]],\n",
       " \n",
       "           [[0.0190, 0.0420, 0.1051,  ..., 0.0450, 0.0436, 0.0592],\n",
       "            [0.0265, 0.0653, 0.0660,  ..., 0.0425, 0.0633, 0.0715],\n",
       "            [0.0240, 0.0535, 0.0603,  ..., 0.0439, 0.0685, 0.0772],\n",
       "            ...,\n",
       "            [0.0221, 0.0634, 0.1163,  ..., 0.0359, 0.0479, 0.0711],\n",
       "            [0.0251, 0.0737, 0.0974,  ..., 0.0250, 0.0321, 0.0405],\n",
       "            [0.0213, 0.0633, 0.1769,  ..., 0.0350, 0.0393, 0.0474]],\n",
       " \n",
       "           [[0.0626, 0.0358, 0.0350,  ..., 0.0968, 0.1346, 0.1000],\n",
       "            [0.0720, 0.1541, 0.0450,  ..., 0.0709, 0.1072, 0.0685],\n",
       "            [0.0659, 0.1350, 0.0353,  ..., 0.0864, 0.1332, 0.0826],\n",
       "            ...,\n",
       "            [0.1130, 0.0704, 0.0673,  ..., 0.0502, 0.0975, 0.0852],\n",
       "            [0.1272, 0.0736, 0.0799,  ..., 0.0529, 0.0725, 0.0654],\n",
       "            [0.1224, 0.0853, 0.0729,  ..., 0.0706, 0.1017, 0.0837]]],\n",
       " \n",
       " \n",
       "          [[[0.0209, 0.0939, 0.0918,  ..., 0.0717, 0.1425, 0.1177],\n",
       "            [0.0176, 0.0818, 0.0862,  ..., 0.0815, 0.1435, 0.1043],\n",
       "            [0.0163, 0.0803, 0.0910,  ..., 0.0858, 0.1363, 0.0995],\n",
       "            ...,\n",
       "            [0.0302, 0.0865, 0.0982,  ..., 0.0606, 0.1273, 0.1084],\n",
       "            [0.0232, 0.0594, 0.0936,  ..., 0.0621, 0.1061, 0.1034],\n",
       "            [0.0307, 0.1001, 0.1196,  ..., 0.0670, 0.1171, 0.1168]],\n",
       " \n",
       "           [[0.0335, 0.0333, 0.0377,  ..., 0.0668, 0.0472, 0.0448],\n",
       "            [0.0211, 0.0197, 0.0244,  ..., 0.0646, 0.0410, 0.0381],\n",
       "            [0.0275, 0.0224, 0.0300,  ..., 0.0691, 0.0417, 0.0371],\n",
       "            ...,\n",
       "            [0.0241, 0.0193, 0.0249,  ..., 0.0836, 0.0457, 0.0411],\n",
       "            [0.0200, 0.0123, 0.0182,  ..., 0.0536, 0.0529, 0.0563],\n",
       "            [0.0328, 0.0158, 0.0318,  ..., 0.0771, 0.0454, 0.0442]],\n",
       " \n",
       "           [[0.0895, 0.0447, 0.1009,  ..., 0.0335, 0.0203, 0.0315],\n",
       "            [0.0859, 0.0434, 0.1083,  ..., 0.0360, 0.0211, 0.0265],\n",
       "            [0.1009, 0.0422, 0.0954,  ..., 0.0411, 0.0238, 0.0294],\n",
       "            ...,\n",
       "            [0.0728, 0.0346, 0.0897,  ..., 0.0320, 0.0178, 0.0245],\n",
       "            [0.0864, 0.0492, 0.1251,  ..., 0.0411, 0.0259, 0.0353],\n",
       "            [0.0737, 0.0329, 0.1372,  ..., 0.0405, 0.0204, 0.0325]],\n",
       " \n",
       "           [[0.0545, 0.0505, 0.0371,  ..., 0.0354, 0.0628, 0.0615],\n",
       "            [0.0629, 0.0430, 0.0515,  ..., 0.0443, 0.0547, 0.0594],\n",
       "            [0.0584, 0.0400, 0.0482,  ..., 0.0399, 0.0505, 0.0604],\n",
       "            ...,\n",
       "            [0.0743, 0.0419, 0.0394,  ..., 0.0553, 0.0875, 0.0788],\n",
       "            [0.0338, 0.0558, 0.0602,  ..., 0.0381, 0.0425, 0.0480],\n",
       "            [0.0359, 0.0540, 0.0679,  ..., 0.0392, 0.0490, 0.0549]]]]]),\n",
       " 'cross_attn_ws_tokens': tensor([[[[0.0211, 0.0211, 0.0210,  ..., 0.0207, 0.0205, 0.0204],\n",
       "           [0.0207, 0.0202, 0.0203,  ..., 0.0211, 0.0211, 0.0212],\n",
       "           [0.0207, 0.0203, 0.0204,  ..., 0.0211, 0.0210, 0.0210],\n",
       "           ...,\n",
       "           [0.0208, 0.0205, 0.0204,  ..., 0.0212, 0.0211, 0.0211],\n",
       "           [0.0206, 0.0204, 0.0203,  ..., 0.0212, 0.0211, 0.0210],\n",
       "           [0.0208, 0.0209, 0.0205,  ..., 0.0211, 0.0209, 0.0207]],\n",
       " \n",
       "          [[0.0204, 0.0211, 0.0203,  ..., 0.0217, 0.0214, 0.0207],\n",
       "           [0.0206, 0.0215, 0.0205,  ..., 0.0216, 0.0212, 0.0204],\n",
       "           [0.0206, 0.0216, 0.0205,  ..., 0.0216, 0.0212, 0.0204],\n",
       "           ...,\n",
       "           [0.0204, 0.0212, 0.0203,  ..., 0.0216, 0.0213, 0.0204],\n",
       "           [0.0207, 0.0216, 0.0207,  ..., 0.0213, 0.0210, 0.0204],\n",
       "           [0.0206, 0.0212, 0.0205,  ..., 0.0215, 0.0213, 0.0207]],\n",
       " \n",
       "          [[0.0211, 0.0220, 0.0211,  ..., 0.0209, 0.0206, 0.0202],\n",
       "           [0.0211, 0.0223, 0.0211,  ..., 0.0208, 0.0203, 0.0199],\n",
       "           [0.0210, 0.0223, 0.0210,  ..., 0.0208, 0.0202, 0.0199],\n",
       "           ...,\n",
       "           [0.0213, 0.0227, 0.0216,  ..., 0.0206, 0.0200, 0.0197],\n",
       "           [0.0213, 0.0226, 0.0215,  ..., 0.0206, 0.0200, 0.0196],\n",
       "           [0.0214, 0.0221, 0.0215,  ..., 0.0207, 0.0203, 0.0202]],\n",
       " \n",
       "          [[0.0207, 0.0203, 0.0208,  ..., 0.0206, 0.0207, 0.0213],\n",
       "           [0.0208, 0.0210, 0.0209,  ..., 0.0208, 0.0207, 0.0211],\n",
       "           [0.0209, 0.0211, 0.0210,  ..., 0.0207, 0.0206, 0.0211],\n",
       "           ...,\n",
       "           [0.0207, 0.0210, 0.0208,  ..., 0.0211, 0.0209, 0.0212],\n",
       "           [0.0207, 0.0204, 0.0206,  ..., 0.0207, 0.0208, 0.0213],\n",
       "           [0.0206, 0.0205, 0.0204,  ..., 0.0211, 0.0212, 0.0215]]]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = validation_step(model=rt1, batch=batch, loss_fn=loss_fn)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8dbf5-0c21-40f9-ad2f-a7756961d03e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ§‘ðŸ¾â€ðŸ³ Prepare Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643f24ee-c2cc-4cd3-bb87-e898490c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, dm, opt, loss_fn, scheduler):\n",
    "    \n",
    "    loss_epoch = np.inf\n",
    "    val_loss = np.inf\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    cer_ = np.inf\n",
    "    wer_ = np.inf\n",
    "    \n",
    "    for e in range(config.EPOCHS):        \n",
    "        running_loss = 0.\n",
    "        num_steps = len(dm.train_dataloader())\n",
    "        \n",
    "        pbar = tqdm(\n",
    "            range(num_steps),\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "            total = num_steps\n",
    "        )\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dm.train_dataloader()):            \n",
    "            pct = 100. * step / num_steps\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {e+1}/{config.EPOCHS} - (Train {pct:.1f}%)\"\n",
    "            )\n",
    "            pbar.update()\n",
    "            \n",
    "            opt.zero_grad()\n",
    "\n",
    "            # training step\n",
    "            loss, logits, self_attn_ws, cross_attn_ws_seq, _ = training_step(\n",
    "                model=model, \n",
    "                batch=batch, \n",
    "                loss_fn=loss_fn\n",
    "            )\n",
    "            \n",
    "            # plot attention weights\n",
    "            plot_attention(\n",
    "                self_attn_ws, \n",
    "                show=False, \n",
    "                pre_fix=\"train_selfattn\", \n",
    "                folder=\"train\",\n",
    "                epoch=e,\n",
    "                wandb_logging=True\n",
    "            )\n",
    "\n",
    "            plot_attention(\n",
    "                cross_attn_ws_seq,\n",
    "                kind=\"cross\", \n",
    "                pre_fix=\"train_crossattn\", \n",
    "                show=False, \n",
    "                folder=\"train\",\n",
    "                epoch=e,\n",
    "                wandb_logging=True\n",
    "            )   \n",
    "            \n",
    "            running_loss += loss.item()         \n",
    "            \n",
    "            # logging\n",
    "            if step % 10 == 0:\n",
    "                pbar.set_postfix(\n",
    "                    train_loss_step=\"{:.04f}\".format(running_loss/(step+1)),\n",
    "                    train_loss=\"{:.04f}\".format(loss_epoch),\n",
    "                    CER=\"{:.04f}\".format(cer_),\n",
    "                    WER=\"{:.04f}\".format(wer_),\n",
    "                    val_loss=\"{:.04f}\".format(val_loss),\n",
    "                )\n",
    "                pbar.update()\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            opt.step()\n",
    "            \n",
    "        loss_epoch = running_loss / len(dm.train_dataloader())   \n",
    "        final_lr_epoch = float(opt.param_groups[0]['lr'])\n",
    "        \n",
    "        # predictions\n",
    "        preds = logits.softmax(dim=-1).argmax(dim=-1)\n",
    "\n",
    "        # decode predictions\n",
    "        preds = model.decode_predictions(\n",
    "            predicted_ids=preds\n",
    "        )\n",
    "\n",
    "        labels = model.decode_predictions(\n",
    "            predicted_ids=batch[\"motor_cmd\"][\"labels\"]\n",
    "        )         \n",
    "            \n",
    "        # log decoded sentenses\n",
    "        with open(config.LOGGING_FILE, \"a\") as f:            \n",
    "            f.write(f\"Epoch #{e+1}\\n\")\n",
    "            f.write(f\"[Train] \\n\")\n",
    "            \n",
    "            pred = preds[0]\n",
    "            label = labels[0]\n",
    "            \n",
    "            cer_ = model.cer_fn(pred, label).item()\n",
    "            wer_ = model.wer_fn(pred, label).item()\n",
    "            f.write(f\"Predicted \\t: {pred}\\n\")\n",
    "            f.write(f\"Actual \\t\\t: {label}\\n\")\n",
    "                \n",
    "        # validation\n",
    "        out = validation_step(model=rt1, batch=batch, loss_fn=loss_fn)\n",
    "        val_loss = out[\"val_loss\"]\n",
    "        \n",
    "        # start scheduling lr after epoch X\n",
    "        # X set to 30 to start us of\n",
    "        if e >=30:\n",
    "            scheduler.step(val_loss)\n",
    "       \n",
    "        # plot attention weights\n",
    "        plot_attention(\n",
    "            out[\"self_attn_ws\"], \n",
    "            show=False, \n",
    "            pre_fix=\"val_selfattn\", \n",
    "            folder=\"val\",\n",
    "            epoch=e,\n",
    "            wandb_logging=True\n",
    "        )\n",
    "\n",
    "        plot_attention(\n",
    "            out[\"cross_attn_ws_seq\"],\n",
    "            kind=\"cross\", \n",
    "            pre_fix=\"val_crossattn\", \n",
    "            show=False, \n",
    "            folder=\"val\",\n",
    "            epoch=e,\n",
    "            wandb_logging=True\n",
    "        )   \n",
    "\n",
    "        # plot_attention(\n",
    "        #     out[\"cross_attn_ws_tokens\"], \n",
    "        #     pre_fix=\"val_crossattn_tokens\", \n",
    "        #     show=False, \n",
    "        #     folder=\"val\",\n",
    "        #     epoch=e,\n",
    "        #     wandb_logging=True\n",
    "        # )   \n",
    "        \n",
    "        # update best score\n",
    "        if val_loss < best_val_loss:\n",
    "            # save checkpoint\n",
    "            path = os.path.join(config.MODEL_PATH, \"be_model.bin\")\n",
    "            torch.save({\n",
    "                'model_state_dict'      :model.state_dict(),\n",
    "                'optimizer_state_dict'  :opt.state_dict(),\n",
    "                'val_loss'              : val_loss, \n",
    "                'epoch'                 : e\n",
    "                }, path)\n",
    "            \n",
    "            # update best score\n",
    "            best_val_loss = val_loss        \n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            train_loss_step=\"{:.04f}\".format(running_loss/(step+1)),\n",
    "            train_loss=\"{:.04f}\".format(loss_epoch),\n",
    "            # CER=\"{:.04f}\".format(cer_),\n",
    "            # WER=\"{:.04f}\".format(wer_),\n",
    "            val_Loss=\"{:.04f}\".format(val_loss),\n",
    "            val_CER=\"{:.04f}\".format(out[\"CER\"]),\n",
    "            val_WER=\"{:.04f}\".format(out[\"WER\"]),\n",
    "            lr_epoch=\"{:.1e}\".format(final_lr_epoch),\n",
    "        )  \n",
    "        pbar.update()\n",
    "        \n",
    "        logs_dict = {\n",
    "            \"train_loss\":loss_epoch,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"val_CER\":out[\"CER\"],\n",
    "            \"valWER\":out[\"WER\"],\n",
    "            \"lr\":final_lr_epoch\n",
    "        }\n",
    "        wandb.log(logs_dict)\n",
    "        \n",
    "        # log decoded sentenses\n",
    "        with open(config.LOGGING_FILE, \"a\") as f:                        \n",
    "            pred = out[\"pred_tokens\"]\n",
    "            label = out[\"label\"]\n",
    "            \n",
    "            f.write(f\"[Val] \\n\")            \n",
    "            f.write(f\"Predicted \\t: {pred}\\n\")\n",
    "            f.write(f\"Actual \\t\\t: {label}\\n\") \n",
    "            f.write(f\"Curr val loss \\t\\t: {val_loss:.5f}\\n\") \n",
    "            f.write(f\"Best loss: \\t\\t: {best_val_loss:.5f}\\n\\n\") \n",
    "            \n",
    "        pbar.close()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffd0fb-8dcb-4d92-b2e9-72be3348c68d",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72979ca-9b33-4af9-b772-9cf39cccb6d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdric225\u001b[0m (\u001b[33mjepsam-s23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis230036p/cmanouan/repo/smf_be/notebooks/wandb/run-20231118_023226-ehu6v7a7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jepsam-s23/SMF-Be/runs/ehu6v7a7' target=\"_blank\">be_model</a></strong> to <a href='https://wandb.ai/jepsam-s23/SMF-Be' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jepsam-s23/SMF-Be' target=\"_blank\">https://wandb.ai/jepsam-s23/SMF-Be</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jepsam-s23/SMF-Be/runs/ehu6v7a7' target=\"_blank\">https://wandb.ai/jepsam-s23/SMF-Be/runs/ehu6v7a7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e93561c51459e986e5d68883f3c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42aaef834a5c4d79bdb6a2516c2be76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf14c08f69974addab12effe9cbcec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init experiment\n",
    "run = wandb.init(\n",
    "    project='SMF-Be', \n",
    "    group=\"RT1-CRAM\", \n",
    "    name=\"be_model\", \n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "with open(config.LOGGING_FILE, \"a\") as f:   \n",
    "    f.write(\"*** New experiment ***\\n\")\n",
    "    \n",
    "    \n",
    "trained_model = run_experiment(\n",
    "    model=rt1, \n",
    "    dm=dm, \n",
    "    opt=opt, \n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa000104-cc3b-4927-ae6e-b7ec8745e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac23076-a4a3-459a-b714-eb4ad052fbb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ‘¨ðŸ¿â€ðŸ”¬ Test / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883c8ac-54da-4758-9382-a7e0c8ec8294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9c86c-0345-4fe7-b85b-07cdb971a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids=batch[\"action_desc\"][\"ids\"].cuda()\n",
    "# attn_mask=batch[\"action_desc\"][\"mask\"].cuda()\n",
    "# token_type_ids=batch[\"action_desc\"][\"token_type_ids\"].cuda()\n",
    "# imgs=batch[\"in_state\"].cuda()\n",
    "# decoder_inp=batch[\"motor_cmd\"][\"decoder_inp_ids\"].cuda()\n",
    "# src_mask=(batch[\"source_mask\"].cuda(), batch[\"source_mask_tokens\"].cuda())\n",
    "# target_mask=batch[\"target_mask\"].cuda()\n",
    "# labels = batch[\"motor_cmd\"][\"labels\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2c917-08ae-444c-a456-fd3ace430e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed396280-b161-4b1f-b8b6-0801e3786cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0a9f9-42d6-4905-bc84-c23b468c7c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246745c7-2958-493c-90b9-94e77b0b85b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMF-BE",
   "language": "python",
   "name": "smf-be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
